{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0273d64b-68d7-4273-a92b-dfd3284377ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import IPython\n",
    "import os\n",
    "import getpass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3e15d0-24af-4b46-b735-ce8ebc1f921f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Google AI API Key ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = getpass.getpass(\"Enter your Google AI API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f1475b-8309-47ae-820b-3cc9338811a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with entity_type='concepts-general', \n",
      "entity_wikipedia='https://en.wikipedia.org/wiki/Large_language_model', \n",
      "entity_root='Large language model', custom_entity_root=False, levels=3, \n",
      "llm_model='gemini/gemini-2.0-flash', llm_temp=0.0, output_folder='./_output/'\n",
      "\n",
      "   0% ------------------------------------- 0/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing Large language model (level 1, total tokens 0)\n",
      "   0% ------------------------------------- 0/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 1:    0% ----------------- 0/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing Transformer (machine learning model) (level 2, total tokens 601)\n",
      "Processing level 1:    0% ----------------- 0/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing Natural Language Processing (level 2, total tokens 1,321)\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing Deep Learning (level 2, total tokens 2,057)\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:00 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing Generative model (level 2, total tokens 2,705)\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing Neural network (level 2, total tokens 3,477)\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Output html: \n",
      "'_output\\concepts-general\\large-language-model\\concepts-general_large-language-\n",
      "model_v1.3.2_gemini-gemini-2.0-flash_level2_incl_unprocessed.html' (level 2)\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing Attention Mechanism (level 3, total tokens 4,181)\n",
      "Processing level 2:   33% ----- ----------- 1/3  [ 0:00:01 < -:--:-- , ? it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:01 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing Neural Machine Translation (level 3, total tokens 4,897)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing Sequence-to-Sequence Model (level 3, total tokens 5,513)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing BERT (Bidirectional Encoder Representations from Transformers) \n",
      "(level 3, total tokens 6,173)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing GPT (Generative Pre-trained Transformer) (level 3, total tokens \n",
      "6,783)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:02 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing Computational Linguistics (level 3, total tokens 7,415)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing Machine Learning (level 3, total tokens 8,041)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:03 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing Artificial Intelligence (level 3, total tokens 8,732)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing Text Mining (level 3, total tokens 9,339)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing Speech Recognition (level 3, total tokens 10,003)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:04 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing Representation Learning (level 3, total tokens 10,662)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing Backpropagation (level 3, total tokens 11,313)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing Convolutional Neural Network (level 3, total tokens 11,983)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:05 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing Discriminative model (level 3, total tokens 12,705)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing Variational autoencoder (level 3, total tokens 13,341)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing Generative adversarial network (level 3, total tokens 13,959)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:06 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing Autoregressive model (level 3, total tokens 14,616)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing Markov chain (level 3, total tokens 15,299)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing Connectionism (level 3, total tokens 15,944)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:07 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing Computational neuroscience (level 3, total tokens 16,552)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing Cognitive science (level 3, total tokens 17,262)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Output html: \n",
      "'_output\\concepts-general\\large-language-model\\concepts-general_large-language-\n",
      "model_v1.3.2_gemini-gemini-2.0-flash_level3_incl_unprocessed.html' (level 3)\n",
      "Processing level 3:   67% ----------- ----- 2/3  [ 0:00:08 < 0:00:02 , 1 it/s ]\n",
      "Processing level 3:  100% ----------------- 3/3  [ 0:00:09 < 0:00:00 , 0 it/s ]\n",
      "\n",
      "llmgraph finished, took 9.039459s.\n",
      "Output written to folder '_output\\concepts-general\\large-language-model' which \n",
      "includes, for each level:\n",
      " - An html file with only processed nodes as a fully connected graph\n",
      " - An html file with both processed and extra unprocessed edge nodes\n",
      " - A .graphml file (see http://graphml.graphdrawing.org/)\n",
      " - A .gefx file, good for viewing in gephi (see https://gexf.net/ and \n",
      "https://gephi.org/)\n",
      "\n",
      "Thank you for using llmgraph! Please consider starring the project on github: \n",
      "https://github.com/dylanhogg/llmgraph\n"
     ]
    }
   ],
   "source": [
    "!llmgraph concepts-general https://en.wikipedia.org/wiki/Large_language_model --levels 3 --llm-model gemini/gemini-2.0-flash --llm-temp 0.0 --no-allow-user-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda32d15-4aa1-46d5-aabd-232a3f85a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_output\\concepts-general\\large-language-model\\concepts-general_large-language-model_v1.3.2_gemini-gemini-2.0-flash_level3_fully_connected.html\n",
      "_output\\concepts-general\\large-language-model\\concepts-general_large-language-model_v1.3.2_gemini-gemini-2.0-flash_level3.graphml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get list of book html files from the _output folder\n",
    "html_files = []\n",
    "graphml_files = []\n",
    "for root, dirs, files in os.walk(\"_output\"):\n",
    "  if not dirs:\n",
    "    html_files.extend([str(Path(root) / f) for f in files if f.endswith(\"fully_connected.html\")])\n",
    "    graphml_files.extend([str(Path(root) / f) for f in files if f.endswith(\".graphml\")])\n",
    "html_files = sorted(html_files)\n",
    "graphml_files = sorted(graphml_files)\n",
    "html_file = html_files[-1]\n",
    "graphml_file = graphml_files[-1]\n",
    "\n",
    "print(html_file)\n",
    "print(graphml_file)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c88fa3-c71c-42d7-8813-bf7385279e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7c1c64-2061-4e73-a05d-bf0289ef8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load graphml file\n",
    "G = nx.read_graphml(graphml_file)\n",
    "\n",
    "# Create pyvis network for displaying\n",
    "nt = Network(height=\"800px\", width=\"100%\", directed=True, cdn_resources=\"remote\", notebook=True)\n",
    "nt.from_nx(G)\n",
    "nt.force_atlas_2based(\n",
    "    spring_strength=0.03\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4671632c-fe27-4b47-9c67-f4672bb903a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "    <head>\n",
       "        <meta charset=\"utf-8\">\n",
       "        \n",
       "            <script>function neighbourhoodHighlight(params) {\n",
       "  // console.log(\"in nieghbourhoodhighlight\");\n",
       "  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "  // originalNodes = JSON.parse(JSON.stringify(allNodes));\n",
       "  // if something is selected:\n",
       "  if (params.nodes.length > 0) {\n",
       "    highlightActive = true;\n",
       "    var i, j;\n",
       "    var selectedNode = params.nodes[0];\n",
       "    var degrees = 2;\n",
       "\n",
       "    // mark all nodes as hard to read.\n",
       "    for (let nodeId in allNodes) {\n",
       "      // nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "      allNodes[nodeId].color = \"rgba(200,200,200,0.5)\";\n",
       "      if (allNodes[nodeId].hiddenLabel === undefined) {\n",
       "        allNodes[nodeId].hiddenLabel = allNodes[nodeId].label;\n",
       "        allNodes[nodeId].label = undefined;\n",
       "      }\n",
       "    }\n",
       "    var connectedNodes = network.getConnectedNodes(selectedNode);\n",
       "    var allConnectedNodes = [];\n",
       "\n",
       "    // get the second degree nodes\n",
       "    for (i = 1; i < degrees; i++) {\n",
       "      for (j = 0; j < connectedNodes.length; j++) {\n",
       "        allConnectedNodes = allConnectedNodes.concat(\n",
       "          network.getConnectedNodes(connectedNodes[j])\n",
       "        );\n",
       "      }\n",
       "    }\n",
       "\n",
       "    // all second degree nodes get a different color and their label back\n",
       "    for (i = 0; i < allConnectedNodes.length; i++) {\n",
       "      // allNodes[allConnectedNodes[i]].color = \"pink\";\n",
       "      allNodes[allConnectedNodes[i]].color = \"rgba(150,150,150,0.75)\";\n",
       "      if (allNodes[allConnectedNodes[i]].hiddenLabel !== undefined) {\n",
       "        allNodes[allConnectedNodes[i]].label =\n",
       "          allNodes[allConnectedNodes[i]].hiddenLabel;\n",
       "        allNodes[allConnectedNodes[i]].hiddenLabel = undefined;\n",
       "      }\n",
       "    }\n",
       "\n",
       "    // all first degree nodes get their own color and their label back\n",
       "    for (i = 0; i < connectedNodes.length; i++) {\n",
       "      // allNodes[connectedNodes[i]].color = undefined;\n",
       "      allNodes[connectedNodes[i]].color = nodeColors[connectedNodes[i]];\n",
       "      if (allNodes[connectedNodes[i]].hiddenLabel !== undefined) {\n",
       "        allNodes[connectedNodes[i]].label =\n",
       "          allNodes[connectedNodes[i]].hiddenLabel;\n",
       "        allNodes[connectedNodes[i]].hiddenLabel = undefined;\n",
       "      }\n",
       "    }\n",
       "\n",
       "    // the main node gets its own color and its label back.\n",
       "    // allNodes[selectedNode].color = undefined;\n",
       "    allNodes[selectedNode].color = nodeColors[selectedNode];\n",
       "    if (allNodes[selectedNode].hiddenLabel !== undefined) {\n",
       "      allNodes[selectedNode].label = allNodes[selectedNode].hiddenLabel;\n",
       "      allNodes[selectedNode].hiddenLabel = undefined;\n",
       "    }\n",
       "  } else if (highlightActive === true) {\n",
       "    // console.log(\"highlightActive was true\");\n",
       "    // reset all nodes\n",
       "    for (let nodeId in allNodes) {\n",
       "      // allNodes[nodeId].color = \"purple\";\n",
       "      allNodes[nodeId].color = nodeColors[nodeId];\n",
       "      // delete allNodes[nodeId].color;\n",
       "      if (allNodes[nodeId].hiddenLabel !== undefined) {\n",
       "        allNodes[nodeId].label = allNodes[nodeId].hiddenLabel;\n",
       "        allNodes[nodeId].hiddenLabel = undefined;\n",
       "      }\n",
       "    }\n",
       "    highlightActive = false;\n",
       "  }\n",
       "\n",
       "  // transform the object into an array\n",
       "  var updateArray = [];\n",
       "  if (params.nodes.length > 0) {\n",
       "    for (let nodeId in allNodes) {\n",
       "      if (allNodes.hasOwnProperty(nodeId)) {\n",
       "        // console.log(allNodes[nodeId]);\n",
       "        updateArray.push(allNodes[nodeId]);\n",
       "      }\n",
       "    }\n",
       "    nodes.update(updateArray);\n",
       "  } else {\n",
       "    // console.log(\"Nothing was selected\");\n",
       "    for (let nodeId in allNodes) {\n",
       "      if (allNodes.hasOwnProperty(nodeId)) {\n",
       "        // console.log(allNodes[nodeId]);\n",
       "        // allNodes[nodeId].color = {};\n",
       "        updateArray.push(allNodes[nodeId]);\n",
       "      }\n",
       "    }\n",
       "    nodes.update(updateArray);\n",
       "  }\n",
       "}\n",
       "\n",
       "function filterHighlight(params) {\n",
       "  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "  // if something is selected:\n",
       "  if (params.nodes.length > 0) {\n",
       "    filterActive = true;\n",
       "    let selectedNodes = params.nodes;\n",
       "\n",
       "    // hiding all nodes and saving the label\n",
       "    for (let nodeId in allNodes) {\n",
       "      allNodes[nodeId].hidden = true;\n",
       "      if (allNodes[nodeId].savedLabel === undefined) {\n",
       "        allNodes[nodeId].savedLabel = allNodes[nodeId].label;\n",
       "        allNodes[nodeId].label = undefined;\n",
       "      }\n",
       "    }\n",
       "\n",
       "    for (let i=0; i < selectedNodes.length; i++) {\n",
       "      allNodes[selectedNodes[i]].hidden = false;\n",
       "      if (allNodes[selectedNodes[i]].savedLabel !== undefined) {\n",
       "        allNodes[selectedNodes[i]].label = allNodes[selectedNodes[i]].savedLabel;\n",
       "        allNodes[selectedNodes[i]].savedLabel = undefined;\n",
       "      }\n",
       "    }\n",
       "\n",
       "  } else if (filterActive === true) {\n",
       "    // reset all nodes\n",
       "    for (let nodeId in allNodes) {\n",
       "      allNodes[nodeId].hidden = false;\n",
       "      if (allNodes[nodeId].savedLabel !== undefined) {\n",
       "        allNodes[nodeId].label = allNodes[nodeId].savedLabel;\n",
       "        allNodes[nodeId].savedLabel = undefined;\n",
       "      }\n",
       "    }\n",
       "    filterActive = false;\n",
       "  }\n",
       "\n",
       "  // transform the object into an array\n",
       "  var updateArray = [];\n",
       "  if (params.nodes.length > 0) {\n",
       "    for (let nodeId in allNodes) {\n",
       "      if (allNodes.hasOwnProperty(nodeId)) {\n",
       "        updateArray.push(allNodes[nodeId]);\n",
       "      }\n",
       "    }\n",
       "    nodes.update(updateArray);\n",
       "  } else {\n",
       "    for (let nodeId in allNodes) {\n",
       "      if (allNodes.hasOwnProperty(nodeId)) {\n",
       "        updateArray.push(allNodes[nodeId]);\n",
       "      }\n",
       "    }\n",
       "    nodes.update(updateArray);\n",
       "  }\n",
       "}\n",
       "\n",
       "function selectNode(nodes) {\n",
       "  network.selectNodes(nodes);\n",
       "  neighbourhoodHighlight({ nodes: nodes });\n",
       "  return nodes;\n",
       "}\n",
       "\n",
       "function selectNodes(nodes) {\n",
       "  network.selectNodes(nodes);\n",
       "  filterHighlight({nodes: nodes});\n",
       "  return nodes;\n",
       "}\n",
       "\n",
       "function highlightFilter(filter) {\n",
       "  let selectedNodes = []\n",
       "  let selectedProp = filter['property']\n",
       "  if (filter['item'] === 'node') {\n",
       "    let allNodes = nodes.get({ returnType: \"Object\" });\n",
       "    for (let nodeId in allNodes) {\n",
       "      if (allNodes[nodeId][selectedProp] && filter['value'].includes((allNodes[nodeId][selectedProp]).toString())) {\n",
       "        selectedNodes.push(nodeId)\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  else if (filter['item'] === 'edge'){\n",
       "    let allEdges = edges.get({returnType: 'object'});\n",
       "    // check if the selected property exists for selected edge and select the nodes connected to the edge\n",
       "    for (let edge in allEdges) {\n",
       "      if (allEdges[edge][selectedProp] && filter['value'].includes((allEdges[edge][selectedProp]).toString())) {\n",
       "        selectedNodes.push(allEdges[edge]['from'])\n",
       "        selectedNodes.push(allEdges[edge]['to'])\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  selectNodes(selectedNodes)\n",
       "}</script>\n",
       "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css\" integrity=\"sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\" />\n",
       "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js\" integrity=\"sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
       "            \n",
       "            \n",
       "            \n",
       "            \n",
       "            \n",
       "            \n",
       "\n",
       "        \n",
       "<center>\n",
       "<h1></h1>\n",
       "</center>\n",
       "\n",
       "<!-- <link rel=\"stylesheet\" href=\"../node_modules/vis/dist/vis.min.css\" type=\"text/css\" />\n",
       "<script type=\"text/javascript\" src=\"../node_modules/vis/dist/vis.js\"> </script>-->\n",
       "        <link\n",
       "          href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css\"\n",
       "          rel=\"stylesheet\"\n",
       "          integrity=\"sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        />\n",
       "        <script\n",
       "          src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js\"\n",
       "          integrity=\"sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf\"\n",
       "          crossorigin=\"anonymous\"\n",
       "        ></script>\n",
       "\n",
       "\n",
       "        <center>\n",
       "          <h1></h1>\n",
       "        </center>\n",
       "        <style type=\"text/css\">\n",
       "\n",
       "             #mynetwork {\n",
       "                 width: 100%;\n",
       "                 height: 800px;\n",
       "                 background-color: #ffffff;\n",
       "                 border: 1px solid lightgray;\n",
       "                 position: relative;\n",
       "                 float: left;\n",
       "             }\n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "\n",
       "             \n",
       "             /* position absolute is important and the container has to be relative or absolute as well. */\n",
       "          div.popup {\n",
       "                 position:absolute;\n",
       "                 top:0px;\n",
       "                 left:0px;\n",
       "                 display:none;\n",
       "                 background-color:#f5f4ed;\n",
       "                 -moz-border-radius: 3px;\n",
       "                 -webkit-border-radius: 3px;\n",
       "                 border-radius: 3px;\n",
       "                 border: 1px solid #808074;\n",
       "                 box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);\n",
       "          }\n",
       "\n",
       "          /* hide the original tooltip */\n",
       "          .vis-tooltip {\n",
       "            display:none;\n",
       "          }\n",
       "             \n",
       "        </style>\n",
       "    </head>\n",
       "\n",
       "\n",
       "    <body>\n",
       "        <div class=\"card\" style=\"width: 100%\">\n",
       "            \n",
       "            \n",
       "            <div id=\"mynetwork\" class=\"card-body\"></div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        <script type=\"text/javascript\">\n",
       "\n",
       "              // initialize global variables.\n",
       "              var edges;\n",
       "              var nodes;\n",
       "              var allNodes;\n",
       "              var allEdges;\n",
       "              var nodeColors;\n",
       "              var originalNodes;\n",
       "              var network;\n",
       "              var container;\n",
       "              var options, data;\n",
       "              var filter = {\n",
       "                  item : '',\n",
       "                  property : '',\n",
       "                  value : []\n",
       "              };\n",
       "\n",
       "              \n",
       "\n",
       "              \n",
       "\n",
       "              // This method is responsible for drawing the graph, returns the drawn network\n",
       "              function drawGraph() {\n",
       "                  var container = document.getElementById('mynetwork');\n",
       "\n",
       "                  \n",
       "\n",
       "                  // parsing and collecting nodes and edges from the python\n",
       "                  nodes = new vis.DataSet([{\"group\": 1, \"id\": \"Large language model\", \"label\": \"Large language model\", \"level\": 1, \"name\": \"Large language model\", \"node_count\": 0, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"0. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Large_language_model\\u0027 target=\\u0027_blank\\u0027\\u003eLarge language model\\u003c/a\\u003e\\u003cbr /\\u003eA large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\u003cbr /\\u003e[200, G1, L1, PR]\", \"wikipedia_canonical\": \"Large_language_model\", \"wikipedia_content\": \"A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Large_language_model\", \"wikipedia_normalized\": \"Large language model\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Transformer (machine learning model)\", \"label\": \"Transformer (machine learning model)\", \"level\": 2, \"name\": \"Transformer (machine learning model)\", \"node_count\": 1, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"1. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (machine learning model)\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (deep learning architecture)\\u003c/a\\u003e\\u003cbr /\\u003eThe transformer is a deep learning architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminis\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Transformer_(deep_learning_architecture)\", \"wikipedia_content\": \"The transformer is a deep learning architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminis\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\", \"wikipedia_normalized\": \"Transformer (deep learning architecture)\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Natural language processing\", \"label\": \"Natural language processing\", \"level\": 2, \"name\": \"Natural Language Processing\", \"node_count\": 2, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"2. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Natural_language_processing\\u0027 target=\\u0027_blank\\u0027\\u003eNatural language processing\\u003c/a\\u003e\\u003cbr /\\u003eNatural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Natural_language_processing\", \"wikipedia_content\": \"Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Natural_language_processing\", \"wikipedia_normalized\": \"Natural language processing\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Deep learning\", \"label\": \"Deep learning\", \"level\": 2, \"name\": \"Deep Learning\", \"node_count\": 3, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"3. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Deep_learning\\u0027 target=\\u0027_blank\\u0027\\u003eDeep learning\\u003c/a\\u003e\\u003cbr /\\u003eDeep learning is a subset of machine learning that focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \\\"training\\\" them to process data. The adjective \\\"deep\\\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Deep_learning\", \"wikipedia_content\": \"Deep learning is a subset of machine learning that focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \\\"training\\\" them to process data. The adjective \\\"deep\\\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Deep_learning\", \"wikipedia_normalized\": \"Deep learning\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Generative model\", \"label\": \"Generative model\", \"level\": 2, \"name\": \"Generative model\", \"node_count\": 4, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"4. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Generative_model\\u0027 target=\\u0027_blank\\u0027\\u003eGenerative model\\u003c/a\\u003e\\u003cbr /\\u003eIn statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished:A generative model is a statistical model of the joint probability distribution  on a given observable variable X and target variable Y; A generative model can be used to \\\"generate\\\" random instances (outcomes\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Generative_model\", \"wikipedia_content\": \"In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished:A generative model is a statistical model of the joint probability distribution  on a given observable variable X and target variable Y; A generative model can be used to \\\"generate\\\" random instances (outcomes\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Generative_model\", \"wikipedia_normalized\": \"Generative model\", \"wikipedia_resp_code\": 200}, {\"group\": 2, \"id\": \"Artificial neural network\", \"label\": \"Artificial neural network\", \"level\": 2, \"name\": \"Neural network\", \"node_count\": 5, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"5. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Artificial_neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eArtificial neural network\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Neural_network_(machine_learning)\\u0027 target=\\u0027_blank\\u0027\\u003eNeural network (machine learning)\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning, a neural network is a computational model inspired by the structure and functions of biological neural networks.\\u003cbr /\\u003e[200, G2, L2, PR]\", \"wikipedia_canonical\": \"Neural_network_(machine_learning)\", \"wikipedia_content\": \"In machine learning, a neural network is a computational model inspired by the structure and functions of biological neural networks.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Artificial_neural_network\", \"wikipedia_normalized\": \"Neural network (machine learning)\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Attention (machine learning)\", \"label\": \"Attention (machine learning)\", \"level\": 3, \"name\": \"Attention Mechanism\", \"node_count\": 6, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"6. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Attention_(machine_learning)\\u0027 target=\\u0027_blank\\u0027\\u003eAttention (machine learning)\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \\\"soft\\\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Attention_(machine_learning)\", \"wikipedia_content\": \"In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \\\"soft\\\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Attention_(machine_learning)\", \"wikipedia_normalized\": \"Attention (machine learning)\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Neural machine translation\", \"label\": \"Neural machine translation\", \"level\": 3, \"name\": \"Neural Machine Translation\", \"node_count\": 7, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"7. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Neural_machine_translation\\u0027 target=\\u0027_blank\\u0027\\u003eNeural machine translation\\u003c/a\\u003e\\u003cbr /\\u003eNeural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Neural_machine_translation\", \"wikipedia_content\": \"Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Neural_machine_translation\", \"wikipedia_normalized\": \"Neural machine translation\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"Sequence-to-sequence model\", \"label\": \"Sequence-to-sequence model\", \"level\": 3, \"name\": \"Sequence-to-Sequence Model\", \"node_count\": 8, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"8. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Sequence-to-sequence_model\\u0027 target=\\u0027_blank\\u0027\\u003eSequence-to-sequence model\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L3, PR]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Sequence-to-sequence_model\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 3, \"id\": \"BERT (language model)\", \"label\": \"BERT (language model)\", \"level\": 3, \"name\": \"BERT (Bidirectional Encoder Representations from Transformers)\", \"node_count\": 9, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"9. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/BERT_(language_model)\\u0027 target=\\u0027_blank\\u0027\\u003eBERT (language model)\\u003c/a\\u003e\\u003cbr /\\u003e \\nBidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state-of-the-art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"BERT_(language_model)\", \"wikipedia_content\": \" \\nBidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. BERT dramatically improved the state-of-the-art for large language models. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/BERT_(language_model)\", \"wikipedia_normalized\": \"BERT (language model)\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"GPT-3\", \"label\": \"GPT-3\", \"level\": 3, \"name\": \"GPT (Generative Pre-trained Transformer)\", \"node_count\": 10, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"10. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/GPT-3\\u0027 target=\\u0027_blank\\u0027\\u003eGPT-3\\u003c/a\\u003e\\u003cbr /\\u003eGenerative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"GPT-3\", \"wikipedia_content\": \"Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/GPT-3\", \"wikipedia_normalized\": \"GPT-3\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Computational linguistics\", \"label\": \"Computational linguistics\", \"level\": 3, \"name\": \"Computational Linguistics\", \"node_count\": 11, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"11. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computational_linguistics\\u0027 target=\\u0027_blank\\u0027\\u003eComputational linguistics\\u003c/a\\u003e\\u003cbr /\\u003eComputational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others. Computational linguistics is closely related to math\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Computational_linguistics\", \"wikipedia_content\": \"Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others. Computational linguistics is closely related to math\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Computational_linguistics\", \"wikipedia_normalized\": \"Computational linguistics\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Machine learning\", \"label\": \"Machine learning\", \"level\": 3, \"name\": \"Machine Learning\", \"node_count\": 12, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"12. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Machine_learning\\u0027 target=\\u0027_blank\\u0027\\u003eMachine learning\\u003c/a\\u003e\\u003cbr /\\u003eMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Machine_learning\", \"wikipedia_content\": \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Machine_learning\", \"wikipedia_normalized\": \"Machine learning\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Artificial intelligence\", \"label\": \"Artificial intelligence\", \"level\": 3, \"name\": \"Artificial Intelligence\", \"node_count\": 13, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"13. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Artificial_intelligence\\u0027 target=\\u0027_blank\\u0027\\u003eArtificial intelligence\\u003c/a\\u003e\\u003cbr /\\u003eArtificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Artificial_intelligence\", \"wikipedia_content\": \"Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\", \"wikipedia_normalized\": \"Artificial intelligence\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Text mining\", \"label\": \"Text mining\", \"level\": 3, \"name\": \"Text Mining\", \"node_count\": 14, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"14. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Text_mining\\u0027 target=\\u0027_blank\\u0027\\u003eText mining\\u003c/a\\u003e\\u003cbr /\\u003eText mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Text_mining\", \"wikipedia_content\": \"Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\" Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Text_mining\", \"wikipedia_normalized\": \"Text mining\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Speech recognition\", \"label\": \"Speech recognition\", \"level\": 3, \"name\": \"Speech Recognition\", \"node_count\": 15, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"15. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Speech_recognition\\u0027 target=\\u0027_blank\\u0027\\u003eSpeech recognition\\u003c/a\\u003e\\u003cbr /\\u003e\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Speech_recognition\", \"wikipedia_content\": \"\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Speech_recognition\", \"wikipedia_normalized\": \"Speech recognition\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Representation learning\", \"label\": \"Representation learning\", \"level\": 3, \"name\": \"Representation Learning\", \"node_count\": 16, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"16. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Representation_learning\\u0027 target=\\u0027_blank\\u0027\\u003eRepresentation learning\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Feature_learning\\u0027 target=\\u0027_blank\\u0027\\u003eFeature learning\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning (ML), feature learning or representation learning is a set of techniques that allow a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Feature_learning\", \"wikipedia_content\": \"In machine learning (ML), feature learning or representation learning is a set of techniques that allow a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Representation_learning\", \"wikipedia_normalized\": \"Feature learning\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Backpropagation\", \"label\": \"Backpropagation\", \"level\": 3, \"name\": \"Backpropagation\", \"node_count\": 17, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"17. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Backpropagation\\u0027 target=\\u0027_blank\\u0027\\u003eBackpropagation\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning, backpropagation is a gradient computation method commonly used for training a neural network to compute its parameter updates.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Backpropagation\", \"wikipedia_content\": \"In machine learning, backpropagation is a gradient computation method commonly used for training a neural network to compute its parameter updates.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Backpropagation\", \"wikipedia_normalized\": \"Backpropagation\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Convolutional neural network\", \"label\": \"Convolutional neural network\", \"level\": 3, \"name\": \"Convolutional Neural Network\", \"node_count\": 18, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"18. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Convolutional_neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eConvolutional neural network\\u003c/a\\u003e\\u003cbr /\\u003eA convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced\\u2014in some cases\\u2014by newer deep learning architectures such as t\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Convolutional_neural_network\", \"wikipedia_content\": \"A convolutional neural network (CNN) is a type of feedforward neural network that learns features via filter optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio. Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced\\u2014in some cases\\u2014by newer deep learning architectures such as t\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Convolutional_neural_network\", \"wikipedia_normalized\": \"Convolutional neural network\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Discriminative model\", \"label\": \"Discriminative model\", \"level\": 3, \"name\": \"Discriminative model\", \"node_count\": 19, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"19. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Discriminative_model\\u0027 target=\\u0027_blank\\u0027\\u003eDiscriminative model\\u003c/a\\u003e\\u003cbr /\\u003eDiscriminative models, also referred to as conditional models, are a class of models frequently used for classification. They are typically used to solve binary classification problems, i.e. assign labels, such as pass/fail, win/lose, alive/dead or healthy/sick, to existing datapoints.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Discriminative_model\", \"wikipedia_content\": \"Discriminative models, also referred to as conditional models, are a class of models frequently used for classification. They are typically used to solve binary classification problems, i.e. assign labels, such as pass/fail, win/lose, alive/dead or healthy/sick, to existing datapoints.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Discriminative_model\", \"wikipedia_normalized\": \"Discriminative model\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Variational autoencoder\", \"label\": \"Variational autoencoder\", \"level\": 3, \"name\": \"Variational autoencoder\", \"node_count\": 20, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"20. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Variational_autoencoder\\u0027 target=\\u0027_blank\\u0027\\u003eVariational autoencoder\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning, a variational autoencoder (VAE) is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling. It is part of the families of probabilistic graphical models and variational Bayesian methods.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Variational_autoencoder\", \"wikipedia_content\": \"In machine learning, a variational autoencoder (VAE) is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling. It is part of the families of probabilistic graphical models and variational Bayesian methods.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Variational_autoencoder\", \"wikipedia_normalized\": \"Variational autoencoder\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Generative adversarial network\", \"label\": \"Generative adversarial network\", \"level\": 3, \"name\": \"Generative adversarial network\", \"node_count\": 21, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"21. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Generative_adversarial_network\\u0027 target=\\u0027_blank\\u0027\\u003eGenerative adversarial network\\u003c/a\\u003e\\u003cbr /\\u003eA generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks compete with each other in the form of a zero-sum game, where one agent\\u0027s gain is another agent\\u0027s loss.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Generative_adversarial_network\", \"wikipedia_content\": \"A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks compete with each other in the form of a zero-sum game, where one agent\\u0027s gain is another agent\\u0027s loss.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Generative_adversarial_network\", \"wikipedia_normalized\": \"Generative adversarial network\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Autoregressive model\", \"label\": \"Autoregressive model\", \"level\": 3, \"name\": \"Autoregressive model\", \"node_count\": 22, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"22. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Autoregressive_model\\u0027 target=\\u0027_blank\\u0027\\u003eAutoregressive model\\u003c/a\\u003e\\u003cbr /\\u003eIn statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it can be used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term ; thus the model is in the form of a stochastic difference equation which should not be confused with a differential equation. Together w\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Autoregressive_model\", \"wikipedia_content\": \"In statistics, econometrics, and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it can be used to describe certain time-varying processes in nature, economics, behavior, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term ; thus the model is in the form of a stochastic difference equation which should not be confused with a differential equation. Together w\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Autoregressive_model\", \"wikipedia_normalized\": \"Autoregressive model\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Markov chain\", \"label\": \"Markov chain\", \"level\": 3, \"name\": \"Markov chain\", \"node_count\": 23, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"23. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Markov_chain\\u0027 target=\\u0027_blank\\u0027\\u003eMarkov chain\\u003c/a\\u003e\\u003cbr /\\u003eIn probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \\\"What happens next depends only on the state of affairs now.\\\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Markov_chain\", \"wikipedia_content\": \"In probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \\\"What happens next depends only on the state of affairs now.\\\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Markov_chain\", \"wikipedia_normalized\": \"Markov chain\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Connectionism\", \"label\": \"Connectionism\", \"level\": 3, \"name\": \"Connectionism\", \"node_count\": 24, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"24. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Connectionism\\u0027 target=\\u0027_blank\\u0027\\u003eConnectionism\\u003c/a\\u003e\\u003cbr /\\u003eConnectionism is an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Connectionism\", \"wikipedia_content\": \"Connectionism is an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Connectionism\", \"wikipedia_normalized\": \"Connectionism\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Computational neuroscience\", \"label\": \"Computational neuroscience\", \"level\": 3, \"name\": \"Computational neuroscience\", \"node_count\": 25, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"25. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computational_neuroscience\\u0027 target=\\u0027_blank\\u0027\\u003eComputational neuroscience\\u003c/a\\u003e\\u003cbr /\\u003eComputational neuroscience is a branch of\\u00a0neuroscience\\u00a0which employs mathematics, computer science, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Computational_neuroscience\", \"wikipedia_content\": \"Computational neuroscience is a branch of\\u00a0neuroscience\\u00a0which employs mathematics, computer science, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Computational_neuroscience\", \"wikipedia_normalized\": \"Computational neuroscience\", \"wikipedia_resp_code\": 200}, {\"group\": 3, \"id\": \"Cognitive science\", \"label\": \"Cognitive science\", \"level\": 3, \"name\": \"Cognitive science\", \"node_count\": 26, \"processed\": 2, \"shape\": \"dot\", \"size\": 10, \"title\": \"26. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Cognitive_science\\u0027 target=\\u0027_blank\\u0027\\u003eCognitive science\\u003c/a\\u003e\\u003cbr /\\u003eCognitive science is the interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition. Mental faculties of concern to cognitive scientists include perception, memory, attention, reasoning, language, and emotion. To understand these faculties, cognitive scientists borrow from fields such as psychology, economics, artificial intelligence, neuroscience, linguistics, and anthropology. The typical analysis of cognitive science sp\\u003cbr /\\u003e[200, G3, L3, PR]\", \"wikipedia_canonical\": \"Cognitive_science\", \"wikipedia_content\": \"Cognitive science is the interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition. Mental faculties of concern to cognitive scientists include perception, memory, attention, reasoning, language, and emotion. To understand these faculties, cognitive scientists borrow from fields such as psychology, economics, artificial intelligence, neuroscience, linguistics, and anthropology. The typical analysis of cognitive science sp\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Cognitive_science\", \"wikipedia_normalized\": \"Cognitive science\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"Self-Attention\", \"label\": \"Self-Attention\", \"level\": 4, \"name\": \"Self-Attention\", \"node_count\": 27, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"27. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Self-Attention\\u0027 target=\\u0027_blank\\u0027\\u003eSelf-Attention\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Self-Attention\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"Sequence-to-sequence learning\", \"label\": \"Sequence-to-sequence learning\", \"level\": 4, \"name\": \"Sequence-to-sequence learning\", \"node_count\": 28, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"28. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Sequence-to-sequence_learning\\u0027 target=\\u0027_blank\\u0027\\u003eSequence-to-sequence learning\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Sequence-to-sequence_learning\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 500, \"id\": \"Memory network\", \"label\": \"Memory network\", \"level\": 4, \"name\": \"Memory Networks\", \"node_count\": 29, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"29. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Memory_network\\u0027 target=\\u0027_blank\\u0027\\u003eMemory network\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Memory_network\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"Machine translation\", \"label\": \"Machine translation\", \"level\": 4, \"name\": \"Machine translation\", \"node_count\": 30, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"30. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Machine_translation\\u0027 target=\\u0027_blank\\u0027\\u003eMachine translation\\u003c/a\\u003e\\u003cbr /\\u003eMachine translation is use of computational techniques to translate text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Machine_translation\", \"wikipedia_content\": \"Machine translation is use of computational techniques to translate text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Machine_translation\", \"wikipedia_normalized\": \"Machine translation\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Recurrent neural network\", \"label\": \"Recurrent neural network\", \"level\": 4, \"name\": \"Recurrent neural network\", \"node_count\": 31, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"31. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Recurrent_neural_network\\u0027 target=\\u0027_blank\\u0027\\u003eRecurrent neural network\\u003c/a\\u003e\\u003cbr /\\u003eRecurrent neural networks (RNNs) are a class of artificial neural networks designed for processing sequential data, such as text, speech, and time series, where the order of elements is important. Unlike feedforward neural networks, which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step. This enables RNNs to capture temporal dependencies and patterns within sequences.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Recurrent_neural_network\", \"wikipedia_content\": \"Recurrent neural networks (RNNs) are a class of artificial neural networks designed for processing sequential data, such as text, speech, and time series, where the order of elements is important. Unlike feedforward neural networks, which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step. This enables RNNs to capture temporal dependencies and patterns within sequences.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Recurrent_neural_network\", \"wikipedia_normalized\": \"Recurrent neural network\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Encoder-decoder model\", \"label\": \"Encoder-decoder model\", \"level\": 4, \"name\": \"Encoder-decoder model\", \"node_count\": 32, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"32. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Encoder-decoder_model\\u0027 target=\\u0027_blank\\u0027\\u003eEncoder-decoder model\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\\u0027 target=\\u0027_blank\\u0027\\u003eTransformer (deep learning architecture)\\u003c/a\\u003e\\u003cbr /\\u003eThe transformer is a deep learning architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminis\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Transformer_(deep_learning_architecture)\", \"wikipedia_content\": \"The transformer is a deep learning architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminis\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Encoder-decoder_model\", \"wikipedia_normalized\": \"Transformer (deep learning architecture)\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Automatic summarization\", \"label\": \"Automatic summarization\", \"level\": 4, \"name\": \"Text summarization\", \"node_count\": 33, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"33. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Automatic_summarization\\u0027 target=\\u0027_blank\\u0027\\u003eAutomatic summarization\\u003c/a\\u003e\\u003cbr /\\u003eAutomatic summarization is the process of shortening a set of data computationally, to create a subset that represents the most important or relevant information within the original content. Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Automatic_summarization\", \"wikipedia_content\": \"Automatic summarization is the process of shortening a set of data computationally, to create a subset that represents the most important or relevant information within the original content. Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Automatic_summarization\", \"wikipedia_normalized\": \"Automatic summarization\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Word embedding\", \"label\": \"Word embedding\", \"level\": 4, \"name\": \"Word embedding\", \"node_count\": 34, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"34. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Word_embedding\\u0027 target=\\u0027_blank\\u0027\\u003eWord embedding\\u003c/a\\u003e\\u003cbr /\\u003eIn natural language processing, a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Word_embedding\", \"wikipedia_content\": \"In natural language processing, a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using language modeling and feature learning techniques, where words or phrases from the vocabulary are mapped to vectors of real numbers.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Word_embedding\", \"wikipedia_normalized\": \"Word embedding\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"Masked language model\", \"label\": \"Masked language model\", \"level\": 4, \"name\": \"Masked language model\", \"node_count\": 35, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"35. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Masked_language_model\\u0027 target=\\u0027_blank\\u0027\\u003eMasked language model\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Masked_language_model\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"Language technology\", \"label\": \"Language technology\", \"level\": 4, \"name\": \"Language Technology\", \"node_count\": 36, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"36. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Language_technology\\u0027 target=\\u0027_blank\\u0027\\u003eLanguage technology\\u003c/a\\u003e\\u003cbr /\\u003eLanguage technology, often called human language technology (HLT), studies methods of how computer programs or electronic devices can analyze, produce, modify or respond to human texts and speech. Working with language technology often requires broad knowledge not only about linguistics but also about computer science. It consists of natural language processing (NLP) and computational linguistics (CL) on the one hand, many application oriented aspects of these, and more low-level aspects such as\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Language_technology\", \"wikipedia_content\": \"Language technology, often called human language technology (HLT), studies methods of how computer programs or electronic devices can analyze, produce, modify or respond to human texts and speech. Working with language technology often requires broad knowledge not only about linguistics but also about computer science. It consists of natural language processing (NLP) and computational linguistics (CL) on the one hand, many application oriented aspects of these, and more low-level aspects such as\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Language_technology\", \"wikipedia_normalized\": \"Language technology\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Corpus linguistics\", \"label\": \"Corpus linguistics\", \"level\": 4, \"name\": \"Corpus Linguistics\", \"node_count\": 37, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"37. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Corpus_linguistics\\u0027 target=\\u0027_blank\\u0027\\u003eCorpus linguistics\\u003c/a\\u003e\\u003cbr /\\u003e\\nCorpus linguistics is an empirical method for the study of language by way of a text corpus. Corpora are balanced, often stratified collections of authentic, \\\"real world\\\", text of speech or writing that aim to represent a given linguistic variety. Today, corpora are generally machine-readable data collections.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Corpus_linguistics\", \"wikipedia_content\": \"\\nCorpus linguistics is an empirical method for the study of language by way of a text corpus. Corpora are balanced, often stratified collections of authentic, \\\"real world\\\", text of speech or writing that aim to represent a given linguistic variety. Today, corpora are generally machine-readable data collections.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Corpus_linguistics\", \"wikipedia_normalized\": \"Corpus linguistics\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Data mining\", \"label\": \"Data mining\", \"level\": 4, \"name\": \"Data Mining\", \"node_count\": 38, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"38. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Data_mining\\u0027 target=\\u0027_blank\\u0027\\u003eData mining\\u003c/a\\u003e\\u003cbr /\\u003eData mining is the process of extracting and finding patterns in massive data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \\\"knowledge discovery in databases\\\" process, or KDD. Aside f\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Data_mining\", \"wikipedia_content\": \"Data mining is the process of extracting and finding patterns in massive data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the \\\"knowledge discovery in databases\\\" process, or KDD. Aside f\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Data_mining\", \"wikipedia_normalized\": \"Data mining\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Statistical learning\", \"label\": \"Statistical learning\", \"level\": 4, \"name\": \"Statistical Learning\", \"node_count\": 39, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"39. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Statistical_learning\\u0027 target=\\u0027_blank\\u0027\\u003eStatistical learning\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Machine_learning\\u0027 target=\\u0027_blank\\u0027\\u003eMachine learning\\u003c/a\\u003e\\u003cbr /\\u003eMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Machine_learning\", \"wikipedia_content\": \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Statistical_learning\", \"wikipedia_normalized\": \"Machine learning\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Pattern recognition\", \"label\": \"Pattern recognition\", \"level\": 4, \"name\": \"Pattern Recognition\", \"node_count\": 40, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"40. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Pattern_recognition\\u0027 target=\\u0027_blank\\u0027\\u003ePattern recognition\\u003c/a\\u003e\\u003cbr /\\u003ePattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess PR capabilities but their primary function is to distinguish and create emergent patterns. PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern re\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Pattern_recognition\", \"wikipedia_content\": \"Pattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess PR capabilities but their primary function is to distinguish and create emergent patterns. PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern re\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Pattern_recognition\", \"wikipedia_normalized\": \"Pattern recognition\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Robotics\", \"label\": \"Robotics\", \"level\": 4, \"name\": \"Robotics\", \"node_count\": 41, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"41. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Robotics\\u0027 target=\\u0027_blank\\u0027\\u003eRobotics\\u003c/a\\u003e\\u003cbr /\\u003eRobotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Robotics\", \"wikipedia_content\": \"Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Robotics\", \"wikipedia_normalized\": \"Robotics\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Cognitive computing\", \"label\": \"Cognitive computing\", \"level\": 4, \"name\": \"Cognitive Computing\", \"node_count\": 42, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"42. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Cognitive_computing\\u0027 target=\\u0027_blank\\u0027\\u003eCognitive computing\\u003c/a\\u003e\\u003cbr /\\u003eCognitive computing refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision, human\\u2013computer interaction, dialog and narrative generation, among other technologies.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Cognitive_computing\", \"wikipedia_content\": \"Cognitive computing refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision, human\\u2013computer interaction, dialog and narrative generation, among other technologies.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Cognitive_computing\", \"wikipedia_normalized\": \"Cognitive computing\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Information retrieval\", \"label\": \"Information retrieval\", \"level\": 4, \"name\": \"Information Retrieval\", \"node_count\": 43, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"43. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Information_retrieval\\u0027 target=\\u0027_blank\\u0027\\u003eInformation retrieval\\u003c/a\\u003e\\u003cbr /\\u003eInformation retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Information_retrieval\", \"wikipedia_content\": \"Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need. The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Information_retrieval\", \"wikipedia_normalized\": \"Information retrieval\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Automatic speech recognition\", \"label\": \"Automatic speech recognition\", \"level\": 4, \"name\": \"Automatic speech recognition\", \"node_count\": 44, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"44. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Automatic_speech_recognition\\u0027 target=\\u0027_blank\\u0027\\u003eAutomatic speech recognition\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Speech_recognition\\u0027 target=\\u0027_blank\\u0027\\u003eSpeech recognition\\u003c/a\\u003e\\u003cbr /\\u003e\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Speech_recognition\", \"wikipedia_content\": \"\\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Automatic_speech_recognition\", \"wikipedia_normalized\": \"Speech recognition\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Voice user interface\", \"label\": \"Voice user interface\", \"level\": 4, \"name\": \"Voice User Interface\", \"node_count\": 45, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"45. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Voice_user_interface\\u0027 target=\\u0027_blank\\u0027\\u003eVoice user interface\\u003c/a\\u003e\\u003cbr /\\u003eA voice-user interface (VUI) enables spoken human interaction with computers, using speech recognition to understand spoken commands and answer questions, and typically text to speech to play a reply. A voice command device is a device controlled with a voice user interface.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Voice_user_interface\", \"wikipedia_content\": \"A voice-user interface (VUI) enables spoken human interaction with computers, using speech recognition to understand spoken commands and answer questions, and typically text to speech to play a reply. A voice command device is a device controlled with a voice user interface.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Voice_user_interface\", \"wikipedia_normalized\": \"Voice user interface\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Speaker recognition\", \"label\": \"Speaker recognition\", \"level\": 4, \"name\": \"Speaker recognition\", \"node_count\": 46, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"46. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Speaker_recognition\\u0027 target=\\u0027_blank\\u0027\\u003eSpeaker recognition\\u003c/a\\u003e\\u003cbr /\\u003e\\nSpeaker recognition is the identification of a person from characteristics of voices. It is used to answer the question \\\"Who is speaking?\\\" The term voice recognition can refer to speaker recognition or speech recognition. Speaker verification contrasts with identification, and speaker recognition differs from speaker diarisation.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Speaker_recognition\", \"wikipedia_content\": \"\\nSpeaker recognition is the identification of a person from characteristics of voices. It is used to answer the question \\\"Who is speaking?\\\" The term voice recognition can refer to speaker recognition or speech recognition. Speaker verification contrasts with identification, and speaker recognition differs from speaker diarisation.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Speaker_recognition\", \"wikipedia_normalized\": \"Speaker recognition\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Acoustic phonetics\", \"label\": \"Acoustic phonetics\", \"level\": 4, \"name\": \"Acoustic Phonetics\", \"node_count\": 47, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"47. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Acoustic_phonetics\\u0027 target=\\u0027_blank\\u0027\\u003eAcoustic phonetics\\u003c/a\\u003e\\u003cbr /\\u003eAcoustic phonetics is a subfield of phonetics, which deals with acoustic aspects of speech sounds. Acoustic phonetics investigates time domain features such as the mean squared amplitude of a waveform, its duration, its fundamental frequency, or frequency domain features such as the frequency spectrum, or even combined spectrotemporal features and the relationship of these properties to other branches of phonetics, and to abstract linguistic concepts such as phonemes, phrases, or utterances.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Acoustic_phonetics\", \"wikipedia_content\": \"Acoustic phonetics is a subfield of phonetics, which deals with acoustic aspects of speech sounds. Acoustic phonetics investigates time domain features such as the mean squared amplitude of a waveform, its duration, its fundamental frequency, or frequency domain features such as the frequency spectrum, or even combined spectrotemporal features and the relationship of these properties to other branches of phonetics, and to abstract linguistic concepts such as phonemes, phrases, or utterances.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Acoustic_phonetics\", \"wikipedia_normalized\": \"Acoustic phonetics\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Feature engineering\", \"label\": \"Feature engineering\", \"level\": 4, \"name\": \"Feature engineering\", \"node_count\": 48, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"48. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Feature_engineering\\u0027 target=\\u0027_blank\\u0027\\u003eFeature engineering\\u003c/a\\u003e\\u003cbr /\\u003eFeature engineering is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Feature_engineering\", \"wikipedia_content\": \"Feature engineering is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Feature_engineering\", \"wikipedia_normalized\": \"Feature engineering\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Unsupervised learning\", \"label\": \"Unsupervised learning\", \"level\": 4, \"name\": \"Unsupervised learning\", \"node_count\": 49, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"49. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Unsupervised_learning\\u0027 target=\\u0027_blank\\u0027\\u003eUnsupervised learning\\u003c/a\\u003e\\u003cbr /\\u003eUnsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data. Other frameworks in the spectrum of supervisions include weak- or semi-supervision, where a small portion of the data is tagged, and self-supervision. Some researchers consider self-supervised learning a form of unsupervised learning.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Unsupervised_learning\", \"wikipedia_content\": \"Unsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data. Other frameworks in the spectrum of supervisions include weak- or semi-supervision, where a small portion of the data is tagged, and self-supervision. Some researchers consider self-supervised learning a form of unsupervised learning.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Unsupervised_learning\", \"wikipedia_normalized\": \"Unsupervised learning\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Dimensionality reduction\", \"label\": \"Dimensionality reduction\", \"level\": 4, \"name\": \"Dimensionality reduction\", \"node_count\": 50, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"50. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Dimensionality_reduction\\u0027 target=\\u0027_blank\\u0027\\u003eDimensionality reduction\\u003c/a\\u003e\\u003cbr /\\u003eDimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimension\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Dimensionality_reduction\", \"wikipedia_content\": \"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimension\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Dimensionality_reduction\", \"wikipedia_normalized\": \"Dimensionality reduction\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Gradient descent\", \"label\": \"Gradient descent\", \"level\": 4, \"name\": \"Gradient Descent\", \"node_count\": 51, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"51. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Gradient_descent\\u0027 target=\\u0027_blank\\u0027\\u003eGradient descent\\u003c/a\\u003e\\u003cbr /\\u003eGradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Gradient_descent\", \"wikipedia_content\": \"Gradient descent is a method for unconstrained mathematical optimization. It is a first-order iterative algorithm for minimizing a differentiable multivariate function.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Gradient_descent\", \"wikipedia_normalized\": \"Gradient descent\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Automatic differentiation\", \"label\": \"Automatic differentiation\", \"level\": 4, \"name\": \"Automatic Differentiation\", \"node_count\": 52, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"52. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Automatic_differentiation\\u0027 target=\\u0027_blank\\u0027\\u003eAutomatic differentiation\\u003c/a\\u003e\\u003cbr /\\u003eIn mathematics and computer algebra, automatic differentiation, also called algorithmic differentiation, computational differentiation, and differentiation arithmetic is a set of techniques to evaluate the partial derivative of a function specified by a computer program. Automatic differentiation is a subtle and central tool to automatize the simultaneous computation of the numerical values of arbitrarily complex functions and their derivatives with no need for the symbolic representation of the\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Automatic_differentiation\", \"wikipedia_content\": \"In mathematics and computer algebra, automatic differentiation, also called algorithmic differentiation, computational differentiation, and differentiation arithmetic is a set of techniques to evaluate the partial derivative of a function specified by a computer program. Automatic differentiation is a subtle and central tool to automatize the simultaneous computation of the numerical values of arbitrarily complex functions and their derivatives with no need for the symbolic representation of the\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Automatic_differentiation\", \"wikipedia_normalized\": \"Automatic differentiation\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Chain rule\", \"label\": \"Chain rule\", \"level\": 4, \"name\": \"Chain Rule\", \"node_count\": 53, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"53. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Chain_rule\\u0027 target=\\u0027_blank\\u0027\\u003eChain rule\\u003c/a\\u003e\\u003cbr /\\u003eIn calculus, the chain rule is a formula that expresses the derivative of the composition of two differentiable functions f and g in terms of the derivatives of f and g. More precisely, if  is the function such that  for every x, then the chain rule is, in Lagrange\\u0027s notation,\\n\\nor, equivalently,\\n\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Chain_rule\", \"wikipedia_content\": \"In calculus, the chain rule is a formula that expresses the derivative of the composition of two differentiable functions f and g in terms of the derivatives of f and g. More precisely, if  is the function such that  for every x, then the chain rule is, in Lagrange\\u0027s notation,\\n\\nor, equivalently,\\n\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Chain_rule\", \"wikipedia_normalized\": \"Chain rule\", \"wikipedia_resp_code\": 200}, {\"group\": 500, \"id\": \"Error surface\", \"label\": \"Error surface\", \"level\": 4, \"name\": \"Error Surface\", \"node_count\": 54, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"54. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Error_surface\\u0027 target=\\u0027_blank\\u0027\\u003eError surface\\u003c/a\\u003e\\u003cbr /\\u003e\\u003cbr /\\u003e[404, G500, L4, UN]\", \"wikipedia_canonical\": \"\", \"wikipedia_content\": \"\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Error_surface\", \"wikipedia_normalized\": \"\", \"wikipedia_resp_code\": 404}, {\"group\": 4, \"id\": \"Image recognition\", \"label\": \"Image recognition\", \"level\": 4, \"name\": \"Image recognition\", \"node_count\": 55, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"55. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Image_recognition\\u0027 target=\\u0027_blank\\u0027\\u003eImage recognition\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computer_vision\\u0027 target=\\u0027_blank\\u0027\\u003eComputer vision\\u003c/a\\u003e\\u003cbr /\\u003eComputer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \\\"Understanding\\\" in this context signifies the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of sy\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Computer_vision\", \"wikipedia_content\": \"Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \\\"Understanding\\\" in this context signifies the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of sy\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Image_recognition\", \"wikipedia_normalized\": \"Computer vision\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Computer vision\", \"label\": \"Computer vision\", \"level\": 4, \"name\": \"Computer vision\", \"node_count\": 56, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"56. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computer_vision\\u0027 target=\\u0027_blank\\u0027\\u003eComputer vision\\u003c/a\\u003e\\u003cbr /\\u003eComputer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \\\"Understanding\\\" in this context signifies the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of sy\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Computer_vision\", \"wikipedia_content\": \"Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \\\"Understanding\\\" in this context signifies the transformation of visual images into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of sy\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Computer_vision\", \"wikipedia_normalized\": \"Computer vision\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Statistical classification\", \"label\": \"Statistical classification\", \"level\": 4, \"name\": \"Classification\", \"node_count\": 57, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"57. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Statistical_classification\\u0027 target=\\u0027_blank\\u0027\\u003eStatistical classification\\u003c/a\\u003e\\u003cbr /\\u003eWhen classification is performed by a computer, statistical methods are normally used to develop the algorithm.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Statistical_classification\", \"wikipedia_content\": \"When classification is performed by a computer, statistical methods are normally used to develop the algorithm.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Statistical_classification\", \"wikipedia_normalized\": \"Statistical classification\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Logistic regression\", \"label\": \"Logistic regression\", \"level\": 4, \"name\": \"Logistic regression\", \"node_count\": 58, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"58. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Logistic_regression\\u0027 target=\\u0027_blank\\u0027\\u003eLogistic regression\\u003c/a\\u003e\\u003cbr /\\u003eIn statistics, a logistic model is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression estimates the parameters of a logistic model. In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \\\"0\\\" and \\\"1\\\", while the independent variables can each be a binary variable or a continuous variable. The correspondi\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Logistic_regression\", \"wikipedia_content\": \"In statistics, a logistic model is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression estimates the parameters of a logistic model. In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \\\"0\\\" and \\\"1\\\", while the independent variables can each be a binary variable or a continuous variable. The correspondi\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Logistic_regression\", \"wikipedia_normalized\": \"Logistic regression\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Support vector machine\", \"label\": \"Support vector machine\", \"level\": 4, \"name\": \"Support Vector Machine\", \"node_count\": 59, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"59. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Support_vector_machine\\u0027 target=\\u0027_blank\\u0027\\u003eSupport vector machine\\u003c/a\\u003e\\u003cbr /\\u003eIn machine learning, support vector machines are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT\\u0026T Bell Laboratories, SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik and Chervonenkis (1974).\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Support_vector_machine\", \"wikipedia_content\": \"In machine learning, support vector machines are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT\\u0026T Bell Laboratories, SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik and Chervonenkis (1974).\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Support_vector_machine\", \"wikipedia_normalized\": \"Support vector machine\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Conditional random field\", \"label\": \"Conditional random field\", \"level\": 4, \"name\": \"Conditional Random Field\", \"node_count\": 60, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"60. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Conditional_random_field\\u0027 target=\\u0027_blank\\u0027\\u003eConditional random field\\u003c/a\\u003e\\u003cbr /\\u003eConditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \\\"neighbouring\\\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. The kind of graph used depends on the application. For exa\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Conditional_random_field\", \"wikipedia_content\": \"Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \\\"neighbouring\\\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. The kind of graph used depends on the application. For exa\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Conditional_random_field\", \"wikipedia_normalized\": \"Conditional random field\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Autoencoder\", \"label\": \"Autoencoder\", \"level\": 4, \"name\": \"Autoencoder\", \"node_count\": 61, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"61. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Autoencoder\\u0027 target=\\u0027_blank\\u0027\\u003eAutoencoder\\u003c/a\\u003e\\u003cbr /\\u003eAn autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data. An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction, to generate lower-dimensional embeddings for subsequent use by other machine learning algorithms.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Autoencoder\", \"wikipedia_content\": \"An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data. An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation. The autoencoder learns an efficient representation (encoding) for a set of data, typically for dimensionality reduction, to generate lower-dimensional embeddings for subsequent use by other machine learning algorithms.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Autoencoder\", \"wikipedia_normalized\": \"Autoencoder\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Latent variable model\", \"label\": \"Latent variable model\", \"level\": 4, \"name\": \"Latent variable model\", \"node_count\": 62, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"62. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Latent_variable_model\\u0027 target=\\u0027_blank\\u0027\\u003eLatent variable model\\u003c/a\\u003e\\u003cbr /\\u003eA latent variable model is a statistical model that relates a set of observable variables to a set of latent variables. Latent variable models are applied across a wide range of fields such as biology, computer science, and social science. Common use cases for latent variable models include applications in psychometrics, and natural language processing.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Latent_variable_model\", \"wikipedia_content\": \"A latent variable model is a statistical model that relates a set of observable variables to a set of latent variables. Latent variable models are applied across a wide range of fields such as biology, computer science, and social science. Common use cases for latent variable models include applications in psychometrics, and natural language processing.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Latent_variable_model\", \"wikipedia_normalized\": \"Latent variable model\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Bayesian network\", \"label\": \"Bayesian network\", \"level\": 4, \"name\": \"Bayesian network\", \"node_count\": 63, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"63. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Bayesian_network\\u0027 target=\\u0027_blank\\u0027\\u003eBayesian network\\u003c/a\\u003e\\u003cbr /\\u003eA Bayesian network is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. For example, a Bayesian network could represent the probabilisti\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Bayesian_network\", \"wikipedia_content\": \"A Bayesian network is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). While it is one of several forms of causal notation, causal networks are special cases of Bayesian networks. Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. For example, a Bayesian network could represent the probabilisti\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Bayesian_network\", \"wikipedia_normalized\": \"Bayesian network\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Principal component analysis\", \"label\": \"Principal component analysis\", \"level\": 4, \"name\": \"Principal component analysis\", \"node_count\": 64, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"64. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Principal_component_analysis\\u0027 target=\\u0027_blank\\u0027\\u003ePrincipal component analysis\\u003c/a\\u003e\\u003cbr /\\u003ePrincipal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Principal_component_analysis\", \"wikipedia_content\": \"Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Principal_component_analysis\", \"wikipedia_normalized\": \"Principal component analysis\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Adversarial machine learning\", \"label\": \"Adversarial machine learning\", \"level\": 4, \"name\": \"Adversarial machine learning\", \"node_count\": 65, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"65. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Adversarial_machine_learning\\u0027 target=\\u0027_blank\\u0027\\u003eAdversarial machine learning\\u003c/a\\u003e\\u003cbr /\\u003eAdversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 revealed practitioners\\u0027 common feeling for better protection of machine learning systems in industrial applications.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Adversarial_machine_learning\", \"wikipedia_content\": \"Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 revealed practitioners\\u0027 common feeling for better protection of machine learning systems in industrial applications.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Adversarial_machine_learning\", \"wikipedia_normalized\": \"Adversarial machine learning\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Markov model\", \"label\": \"Markov model\", \"level\": 4, \"name\": \"Markov model\", \"node_count\": 66, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"66. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Markov_model\\u0027 target=\\u0027_blank\\u0027\\u003eMarkov model\\u003c/a\\u003e\\u003cbr /\\u003eIn probability theory, a Markov model is a stochastic model used to model pseudo-randomly changing systems. It is assumed that future states depend only on the current state, not on the events that occurred before it. Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable. For this reason, in the fields of predictive modelling and probabilistic forecasting, it is desirable for a given model to exhibit the Markov property.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Markov_model\", \"wikipedia_content\": \"In probability theory, a Markov model is a stochastic model used to model pseudo-randomly changing systems. It is assumed that future states depend only on the current state, not on the events that occurred before it. Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable. For this reason, in the fields of predictive modelling and probabilistic forecasting, it is desirable for a given model to exhibit the Markov property.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Markov_model\", \"wikipedia_normalized\": \"Markov model\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Time series analysis\", \"label\": \"Time series analysis\", \"level\": 4, \"name\": \"Time series analysis\", \"node_count\": 67, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"67. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Time_series_analysis\\u0027 target=\\u0027_blank\\u0027\\u003eTime series analysis\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Time_series\\u0027 target=\\u0027_blank\\u0027\\u003eTime series\\u003c/a\\u003e\\u003cbr /\\u003eIn mathematics, a time series is a series of data points indexed in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Time_series\", \"wikipedia_content\": \"In mathematics, a time series is a series of data points indexed in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Time_series_analysis\", \"wikipedia_normalized\": \"Time series\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Moving-average model\", \"label\": \"Moving-average model\", \"level\": 4, \"name\": \"Moving-average model\", \"node_count\": 68, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"68. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Moving-average_model\\u0027 target=\\u0027_blank\\u0027\\u003eMoving-average model\\u003c/a\\u003e\\u003cbr /\\u003eIn time series analysis, the moving-average model, also known as moving-average process, is a common approach for modeling univariate time series. The moving-average model specifies that the output variable is cross-correlated with a non-identical to itself random-variable.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Moving-average_model\", \"wikipedia_content\": \"In time series analysis, the moving-average model, also known as moving-average process, is a common approach for modeling univariate time series. The moving-average model specifies that the output variable is cross-correlated with a non-identical to itself random-variable.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Moving-average_model\", \"wikipedia_normalized\": \"Moving-average model\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Kalman filter\", \"label\": \"Kalman filter\", \"level\": 4, \"name\": \"Kalman filter\", \"node_count\": 69, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"69. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Kalman_filter\\u0027 target=\\u0027_blank\\u0027\\u003eKalman filter\\u003c/a\\u003e\\u003cbr /\\u003eIn statistics and control theory, Kalman filtering is an algorithm that uses a series of measurements observed over time, including statistical noise and other inaccuracies, to produce estimates of unknown variables that tend to be more accurate than those based on a single measurement, by estimating a joint probability distribution over the variables for each time-step. The filter is constructed as a mean squared error minimiser, but an alternative derivation of the filter is also provided show\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Kalman_filter\", \"wikipedia_content\": \"In statistics and control theory, Kalman filtering is an algorithm that uses a series of measurements observed over time, including statistical noise and other inaccuracies, to produce estimates of unknown variables that tend to be more accurate than those based on a single measurement, by estimating a joint probability distribution over the variables for each time-step. The filter is constructed as a mean squared error minimiser, but an alternative derivation of the filter is also provided show\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Kalman_filter\", \"wikipedia_normalized\": \"Kalman filter\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Hidden Markov model\", \"label\": \"Hidden Markov model\", \"level\": 4, \"name\": \"Hidden Markov Model\", \"node_count\": 70, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"70. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Hidden_Markov_model\\u0027 target=\\u0027_blank\\u0027\\u003eHidden Markov model\\u003c/a\\u003e\\u003cbr /\\u003eA hidden Markov model (HMM) is a Markov model in which the observations are dependent on a latent Markov process. An HMM requires that there be an observable process  whose outcomes depend on the outcomes of  in a known way. Since  cannot be observed directly, the goal is to learn about state of  by observing . By definition of being a Markov model, an HMM has an additional requirement that the outcome of  at time  must be \\\"influenced\\\" exclusively by the outcome of  at  and that the outcomes of \\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Hidden_Markov_model\", \"wikipedia_content\": \"A hidden Markov model (HMM) is a Markov model in which the observations are dependent on a latent Markov process. An HMM requires that there be an observable process  whose outcomes depend on the outcomes of  in a known way. Since  cannot be observed directly, the goal is to learn about state of  by observing . By definition of being a Markov model, an HMM has an additional requirement that the outcome of  at time  must be \\\"influenced\\\" exclusively by the outcome of  at  and that the outcomes of \", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Hidden_Markov_model\", \"wikipedia_normalized\": \"Hidden Markov model\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Markov process\", \"label\": \"Markov process\", \"level\": 4, \"name\": \"Markov Process\", \"node_count\": 71, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"71. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Markov_process\\u0027 target=\\u0027_blank\\u0027\\u003eMarkov process\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Markov_chain\\u0027 target=\\u0027_blank\\u0027\\u003eMarkov chain\\u003c/a\\u003e\\u003cbr /\\u003eIn probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \\\"What happens next depends only on the state of affairs now.\\\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Markov_chain\", \"wikipedia_content\": \"In probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \\\"What happens next depends only on the state of affairs now.\\\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Markov_process\", \"wikipedia_normalized\": \"Markov chain\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Stochastic process\", \"label\": \"Stochastic process\", \"level\": 4, \"name\": \"Stochastic Process\", \"node_count\": 72, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"72. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Stochastic_process\\u0027 target=\\u0027_blank\\u0027\\u003eStochastic process\\u003c/a\\u003e\\u003cbr /\\u003eIn probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables in a probability space, where the index of the family often has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas m\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Stochastic_process\", \"wikipedia_content\": \"In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables in a probability space, where the index of the family often has the interpretation of time. Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner. Examples include the growth of a bacterial population, an electrical current fluctuating due to thermal noise, or the movement of a gas m\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Stochastic_process\", \"wikipedia_normalized\": \"Stochastic process\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Queueing theory\", \"label\": \"Queueing theory\", \"level\": 4, \"name\": \"Queueing Theory\", \"node_count\": 73, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"73. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Queueing_theory\\u0027 target=\\u0027_blank\\u0027\\u003eQueueing theory\\u003c/a\\u003e\\u003cbr /\\u003eQueueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Queueing_theory\", \"wikipedia_content\": \"Queueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Queueing_theory\", \"wikipedia_normalized\": \"Queueing theory\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Parallel distributed processing\", \"label\": \"Parallel distributed processing\", \"level\": 4, \"name\": \"Parallel distributed processing\", \"node_count\": 74, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"74. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Parallel_distributed_processing\\u0027 target=\\u0027_blank\\u0027\\u003eParallel distributed processing\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Connectionism\\u0027 target=\\u0027_blank\\u0027\\u003eConnectionism\\u003c/a\\u003e\\u003cbr /\\u003eConnectionism is an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Connectionism\", \"wikipedia_content\": \"Connectionism is an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Parallel_distributed_processing\", \"wikipedia_normalized\": \"Connectionism\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Cognitive psychology\", \"label\": \"Cognitive psychology\", \"level\": 4, \"name\": \"Cognitive psychology\", \"node_count\": 75, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"75. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Cognitive_psychology\\u0027 target=\\u0027_blank\\u0027\\u003eCognitive psychology\\u003c/a\\u003e\\u003cbr /\\u003eCognitive psychology is the scientific study of human mental processes such as attention, language use, memory, perception, problem solving, creativity, and reasoning.\\nCognitive psychology originated in the 1960s in a break from behaviorism, which held from the 1920s to 1950s that unobservable mental processes were outside the realm of empirical science. This break came as researchers in linguistics and cybernetics, as well as applied psychology, used models of mental processing to explain human\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Cognitive_psychology\", \"wikipedia_content\": \"Cognitive psychology is the scientific study of human mental processes such as attention, language use, memory, perception, problem solving, creativity, and reasoning.\\nCognitive psychology originated in the 1960s in a break from behaviorism, which held from the 1920s to 1950s that unobservable mental processes were outside the realm of empirical science. This break came as researchers in linguistics and cybernetics, as well as applied psychology, used models of mental processing to explain human\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Cognitive_psychology\", \"wikipedia_normalized\": \"Cognitive psychology\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Theoretical neuroscience\", \"label\": \"Theoretical neuroscience\", \"level\": 4, \"name\": \"Theoretical neuroscience\", \"node_count\": 76, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"76. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Theoretical_neuroscience\\u0027 target=\\u0027_blank\\u0027\\u003eTheoretical neuroscience\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computational_neuroscience\\u0027 target=\\u0027_blank\\u0027\\u003eComputational neuroscience\\u003c/a\\u003e\\u003cbr /\\u003eComputational neuroscience is a branch of\\u00a0neuroscience\\u00a0which employs mathematics, computer science, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Computational_neuroscience\", \"wikipedia_content\": \"Computational neuroscience is a branch of\\u00a0neuroscience\\u00a0which employs mathematics, computer science, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Theoretical_neuroscience\", \"wikipedia_normalized\": \"Computational neuroscience\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Neuroinformatics\", \"label\": \"Neuroinformatics\", \"level\": 4, \"name\": \"Neuroinformatics\", \"node_count\": 77, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"77. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Neuroinformatics\\u0027 target=\\u0027_blank\\u0027\\u003eNeuroinformatics\\u003c/a\\u003e\\u003cbr /\\u003eNeuroinformatics is the emergent field that combines informatics and neuroscience. Neuroinformatics is related with neuroscience data and information processing by artificial neural networks. There are three main directions where neuroinformatics has to be applied:the development of computational models of the nervous system and neural processes;\\nthe development of tools for analyzing and modeling neuroscience data; and\\nthe development of tools and databases for management and sharing of neurosc\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Neuroinformatics\", \"wikipedia_content\": \"Neuroinformatics is the emergent field that combines informatics and neuroscience. Neuroinformatics is related with neuroscience data and information processing by artificial neural networks. There are three main directions where neuroinformatics has to be applied:the development of computational models of the nervous system and neural processes;\\nthe development of tools for analyzing and modeling neuroscience data; and\\nthe development of tools and databases for management and sharing of neurosc\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Neuroinformatics\", \"wikipedia_normalized\": \"Neuroinformatics\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Cognitive neuroscience\", \"label\": \"Cognitive neuroscience\", \"level\": 4, \"name\": \"Cognitive neuroscience\", \"node_count\": 78, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"78. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Cognitive_neuroscience\\u0027 target=\\u0027_blank\\u0027\\u003eCognitive neuroscience\\u003c/a\\u003e\\u003cbr /\\u003eCognitive neuroscience is the scientific field that is concerned with the study of the biological processes and aspects that underlie cognition, with a specific focus on the neural connections in the brain which are involved in mental processes. It addresses the questions of how cognitive activities are affected or controlled by neural circuits in the brain. Cognitive neuroscience is a branch of both neuroscience and psychology, overlapping with disciplines such as behavioral neuroscience, cogni\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Cognitive_neuroscience\", \"wikipedia_content\": \"Cognitive neuroscience is the scientific field that is concerned with the study of the biological processes and aspects that underlie cognition, with a specific focus on the neural connections in the brain which are involved in mental processes. It addresses the questions of how cognitive activities are affected or controlled by neural circuits in the brain. Cognitive neuroscience is a branch of both neuroscience and psychology, overlapping with disciplines such as behavioral neuroscience, cogni\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Cognitive_neuroscience\", \"wikipedia_normalized\": \"Cognitive neuroscience\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Mathematical neuroscience\", \"label\": \"Mathematical neuroscience\", \"level\": 4, \"name\": \"Mathematical neuroscience\", \"node_count\": 79, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"79. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Mathematical_neuroscience\\u0027 target=\\u0027_blank\\u0027\\u003eMathematical neuroscience\\u003c/a\\u003e \\u2192 \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Computational_neuroscience\\u0027 target=\\u0027_blank\\u0027\\u003eComputational neuroscience\\u003c/a\\u003e\\u003cbr /\\u003eComputational neuroscience is a branch of\\u00a0neuroscience\\u00a0which employs mathematics, computer science, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Computational_neuroscience\", \"wikipedia_content\": \"Computational neuroscience is a branch of\\u00a0neuroscience\\u00a0which employs mathematics, computer science, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Mathematical_neuroscience\", \"wikipedia_normalized\": \"Computational neuroscience\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Psychology\", \"label\": \"Psychology\", \"level\": 4, \"name\": \"Psychology\", \"node_count\": 80, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"80. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Psychology\\u0027 target=\\u0027_blank\\u0027\\u003ePsychology\\u003c/a\\u003e\\u003cbr /\\u003ePsychology is the scientific study of mind and behavior. Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psycho\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Psychology\", \"wikipedia_content\": \"Psychology is the scientific study of mind and behavior. Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psycho\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Psychology\", \"wikipedia_normalized\": \"Psychology\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Neuroscience\", \"label\": \"Neuroscience\", \"level\": 4, \"name\": \"Neuroscience\", \"node_count\": 81, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"81. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Neuroscience\\u0027 target=\\u0027_blank\\u0027\\u003eNeuroscience\\u003c/a\\u003e\\u003cbr /\\u003eNeuroscience is the scientific study of the nervous system, its functions, and its disorders. It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, psychology, physics, computer science, chemistry, medicine, statistics, and mathematical modeling to understand the fundamental and emergent properties of neurons, glia and neural circuits. The understanding of the biological basis of learning, memory, behavior, perception, and consci\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Neuroscience\", \"wikipedia_content\": \"Neuroscience is the scientific study of the nervous system, its functions, and its disorders. It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, psychology, physics, computer science, chemistry, medicine, statistics, and mathematical modeling to understand the fundamental and emergent properties of neurons, glia and neural circuits. The understanding of the biological basis of learning, memory, behavior, perception, and consci\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Neuroscience\", \"wikipedia_normalized\": \"Neuroscience\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Linguistics\", \"label\": \"Linguistics\", \"level\": 4, \"name\": \"Linguistics\", \"node_count\": 82, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"82. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Linguistics\\u0027 target=\\u0027_blank\\u0027\\u003eLinguistics\\u003c/a\\u003e\\u003cbr /\\u003eLinguistics is the scientific study of language. The areas of linguistic analysis are syntax, semantics (meaning), morphology, phonetics, phonology, and pragmatics. Subdisciplines such as biolinguistics and psycholinguistics bridge many of these divisions.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Linguistics\", \"wikipedia_content\": \"Linguistics is the scientific study of language. The areas of linguistic analysis are syntax, semantics (meaning), morphology, phonetics, phonology, and pragmatics. Subdisciplines such as biolinguistics and psycholinguistics bridge many of these divisions.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Linguistics\", \"wikipedia_normalized\": \"Linguistics\", \"wikipedia_resp_code\": 200}, {\"group\": 4, \"id\": \"Philosophy of mind\", \"label\": \"Philosophy of mind\", \"level\": 4, \"name\": \"Philosophy of Mind\", \"node_count\": 83, \"processed\": 0, \"shape\": \"dot\", \"size\": 10, \"title\": \"83. \\u003ca href=\\u0027https://en.wikipedia.org/wiki/Philosophy_of_mind\\u0027 target=\\u0027_blank\\u0027\\u003ePhilosophy of mind\\u003c/a\\u003e\\u003cbr /\\u003ePhilosophy of mind is a branch of philosophy that deals with the nature of the mind and its relation to the body and the external world.\\u003cbr /\\u003e[200, G4, L4, UN]\", \"wikipedia_canonical\": \"Philosophy_of_mind\", \"wikipedia_content\": \"Philosophy of mind is a branch of philosophy that deals with the nature of the mind and its relation to the body and the external world.\", \"wikipedia_link\": \"https://en.wikipedia.org/wiki/Philosophy_of_mind\", \"wikipedia_normalized\": \"Philosophy of mind\", \"wikipedia_resp_code\": 200}]);\n",
       "                  edges = new vis.DataSet([{\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Large language models are almost exclusively based on the Transformer architecture. Transformers provide the architectural foundation for LLMs\\u0027 ability to process sequential data and generate text.\", \"similarity\": 0.95, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Large language models are a key technology within the field of Natural Language Processing. They are used to perform many NLP tasks, such as text generation, translation, and question answering.\", \"similarity\": 0.85, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Large language models are a type of deep learning model, utilizing deep neural networks with many layers to learn complex patterns from data.\", \"similarity\": 0.8, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Large language models are generative models, meaning they are trained to generate new data (text) that is similar to the data they were trained on.\", \"similarity\": 0.75, \"to\": \"Generative model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Large language model\", \"reason\": \"Large language models are a specific type of neural network, characterized by their large size and training on massive datasets. They leverage the principles of neural networks for learning and prediction.\", \"similarity\": 0.7, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"The attention mechanism is the core component of the Transformer architecture. Transformers rely heavily on attention to weigh the importance of different parts of the input sequence when processing it. Without attention, the Transformer model would not function.\", \"similarity\": 0.95, \"to\": \"Attention (machine learning)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"Transformers were initially developed to improve Neural Machine Translation (NMT) and achieved state-of-the-art results in this area. NMT is a primary application of Transformers, and many Transformer architectures are designed with translation tasks in mind.\", \"similarity\": 0.85, \"to\": \"Neural machine translation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"Transformers are a type of sequence-to-sequence model, meaning they take a sequence as input and produce another sequence as output. While traditional sequence-to-sequence models often use recurrent neural networks (RNNs), Transformers offer an alternative approach using attention mechanisms.\", \"similarity\": 0.8, \"to\": \"Sequence-to-sequence model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"BERT is a specific Transformer-based model designed for pre-training on large amounts of text data. It leverages the Transformer architecture to learn contextualized word embeddings, which can then be fine-tuned for various downstream tasks. BERT is a direct descendant and application of the Transformer architecture.\", \"similarity\": 0.9, \"to\": \"BERT (language model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Transformer (machine learning model)\", \"reason\": \"GPT is another Transformer-based model focused on generative tasks, particularly text generation. Like BERT, it is pre-trained on a massive dataset and then fine-tuned for specific applications. GPT showcases the versatility of the Transformer architecture beyond translation, demonstrating its effectiveness in language modeling and text generation.\", \"similarity\": 0.88, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Computational linguistics is a closely related field that uses computational techniques to analyze and process natural language. It focuses on the formal modeling of language and the development of algorithms for language understanding and generation, sharing many of the same goals and methods as NLP.\", \"similarity\": 0.9, \"to\": \"Computational linguistics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Machine learning is a core technology used in many NLP tasks. Modern NLP heavily relies on machine learning algorithms, particularly deep learning, for tasks like text classification, machine translation, and sentiment analysis. Machine learning provides the tools and techniques for NLP systems to learn from data and improve their performance.\", \"similarity\": 0.85, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"NLP is a subfield of AI focused on enabling computers to understand, interpret, and generate human language. It\\u0027s a key component in building intelligent systems that can interact with humans in a natural and intuitive way. NLP contributes to the broader goal of creating machines that can perform tasks that typically require human intelligence.\", \"similarity\": 0.8, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Text mining, also known as text data mining, is the process of extracting valuable information and knowledge from unstructured text data. It uses NLP techniques to analyze text and identify patterns, trends, and relationships. Text mining is often used for tasks like sentiment analysis, topic modeling, and information retrieval, which are also common applications of NLP.\", \"similarity\": 0.75, \"to\": \"Text mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Natural language processing\", \"reason\": \"Speech recognition, also known as automatic speech recognition (ASR), is the process of converting spoken language into text. It is closely related to NLP because the output of speech recognition systems is often used as input for NLP tasks. Furthermore, many of the techniques used in speech recognition, such as acoustic modeling and language modeling, are also used in NLP.\", \"similarity\": 0.7, \"to\": \"Speech recognition\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Deep learning is a subset of neural networks, specifically those with multiple layers (deep neural networks). They share the same fundamental building blocks (neurons, weights, activation functions) and learning paradigms (backpropagation).\", \"similarity\": 0.9, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Deep learning is a subfield of machine learning. Both involve algorithms that learn from data without explicit programming. Deep learning provides a specific approach to machine learning, often excelling in complex pattern recognition tasks.\", \"similarity\": 0.8, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Deep learning excels at representation learning, automatically discovering useful features from raw data. This is a core goal of representation learning, and deep learning provides powerful tools to achieve it.\", \"similarity\": 0.75, \"to\": \"Representation learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Backpropagation is a key algorithm used to train deep neural networks. It\\u0027s the primary method for adjusting the weights of the network based on the error in its predictions. While not exclusive to deep learning, it\\u0027s essential for its success.\", \"similarity\": 0.7, \"to\": \"Backpropagation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Deep learning\", \"reason\": \"Convolutional Neural Networks (CNNs) are a specific type of deep neural network particularly well-suited for processing data with a grid-like topology, such as images. They are a very common and successful application of deep learning.\", \"similarity\": 0.7, \"to\": \"Convolutional neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative model\", \"reason\": \"Discriminative models are the counterpart to generative models. Both are machine learning approaches, but discriminative models learn the conditional probability distribution P(y|x), while generative models learn the joint probability distribution P(x, y). They are often compared and contrasted in the context of classification and regression tasks.\", \"similarity\": 0.85, \"to\": \"Discriminative model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative model\", \"reason\": \"Variational autoencoders (VAEs) are a type of generative model, specifically a probabilistic directed graphical model. They learn a latent space representation of the data and can then generate new samples by sampling from this latent space and decoding it. They are a concrete implementation of generative modeling using neural networks.\", \"similarity\": 0.8, \"to\": \"Variational autoencoder\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative model\", \"reason\": \"Generative adversarial networks (GANs) are another type of generative model that uses a system of two neural networks (a generator and a discriminator) to learn the data distribution and generate new samples. They are a popular and powerful approach to generative modeling, particularly for image generation.\", \"similarity\": 0.75, \"to\": \"Generative adversarial network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative model\", \"reason\": \"Autoregressive models predict future values based on past values. In the context of generative modeling, they can be used to generate sequences of data, such as text or audio, by predicting the next element in the sequence based on the previous elements. They model the conditional probability of each element given the preceding elements.\", \"similarity\": 0.7, \"to\": \"Autoregressive model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative model\", \"reason\": \"Markov chains are stochastic models that describe a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. They can be used as generative models to simulate sequences of states, where each state is generated based on the transition probabilities from the previous state. Hidden Markov Models (HMMs) are a specific type of Markov model often used for generative sequence modeling.\", \"similarity\": 0.65, \"to\": \"Markov chain\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Deep learning is a subfield of machine learning based on artificial neural networks with representation learning. Deep learning architectures such as deep neural networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\", \"similarity\": 0.95, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Artificial neural networks are a core component of many machine learning algorithms. Machine learning is a broader field that encompasses various techniques for enabling computers to learn from data without explicit programming, and neural networks are a powerful tool within this field.\", \"similarity\": 0.85, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Connectionism is an approach in the fields of cognitive science and artificial intelligence that uses artificial neural networks to explain mental phenomena. It emphasizes the interconnectedness of simple units (neurons) to produce complex behavior, mirroring the structure and function of the brain.\", \"similarity\": 0.8, \"to\": \"Connectionism\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Computational neuroscience uses mathematical models and computer simulations to study the nervous system. Artificial neural networks are often used as simplified models of biological neural networks to understand how the brain processes information.\", \"similarity\": 0.75, \"to\": \"Computational neuroscience\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial neural network\", \"reason\": \"Cognitive science is the interdisciplinary study of mind and intelligence, which includes philosophy, neuroscience, artificial intelligence, linguistics, anthropology, psychology. Artificial neural networks are used as models of cognition and learning within cognitive science to understand how the brain performs cognitive tasks.\", \"similarity\": 0.7, \"to\": \"Cognitive science\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Attention (machine learning)\", \"reason\": \"Transformers heavily rely on attention mechanisms as their core building block. The self-attention mechanism within transformers allows the model to weigh the importance of different parts of the input sequence when processing it, making it a direct application and extension of the attention concept.\", \"similarity\": 0.95, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Attention (machine learning)\", \"reason\": \"Self-attention is a specific type of attention mechanism where the attention is applied to relate different positions of the *same* input sequence. It\\u0027s a fundamental component of transformers and a key concept in understanding how attention is used to capture relationships within data.\", \"similarity\": 0.9, \"to\": \"Self-Attention\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Attention (machine learning)\", \"reason\": \"Attention mechanisms were initially popularized and widely adopted in the field of Neural Machine Translation (NMT). They address the limitations of earlier sequence-to-sequence models by allowing the decoder to focus on relevant parts of the input sentence during translation. Attention significantly improved the performance of NMT systems.\", \"similarity\": 0.8, \"to\": \"Neural machine translation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Attention (machine learning)\", \"reason\": \"Attention mechanisms are often used within sequence-to-sequence models to improve their ability to handle long-range dependencies. While sequence-to-sequence models can exist without attention, the integration of attention significantly enhances their performance, especially in tasks like machine translation and text summarization.\", \"similarity\": 0.75, \"to\": \"Sequence-to-sequence learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Attention (machine learning)\", \"reason\": \"Memory Networks use an attention mechanism to select relevant information from an external memory store. This allows the model to reason over a larger context and improve performance on tasks that require long-term memory. The attention mechanism is crucial for retrieving the most relevant memories.\", \"similarity\": 0.7, \"to\": \"Memory network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural machine translation\", \"reason\": \"Machine translation is the broader field that neural machine translation falls under. NMT is a specific approach to machine translation.\", \"similarity\": 0.95, \"to\": \"Machine translation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural machine translation\", \"reason\": \"NMT models are typically based on sequence-to-sequence learning architectures, which are designed to map one sequence to another. This is the fundamental architecture used in NMT.\", \"similarity\": 0.9, \"to\": \"Sequence-to-sequence learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural machine translation\", \"reason\": \"Neural machine translation relies heavily on deep learning techniques, particularly recurrent neural networks (RNNs) and transformers, to learn complex patterns in language.\", \"similarity\": 0.85, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural machine translation\", \"reason\": \"RNNs, especially LSTMs and GRUs, were foundational to early NMT models for handling sequential data. While transformers are now more common, RNNs remain relevant in the history and understanding of NMT.\", \"similarity\": 0.8, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Neural machine translation\", \"reason\": \"The Transformer architecture has become the dominant approach in NMT, significantly improving performance compared to RNN-based models. It addresses limitations of RNNs in capturing long-range dependencies.\", \"similarity\": 0.8, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Sequence-to-sequence model\", \"reason\": \"Neural machine translation is a primary application of sequence-to-sequence models, using them to translate text from one language to another. It shares the core architecture and training methodologies.\", \"similarity\": 0.9, \"to\": \"Neural machine translation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Sequence-to-sequence model\", \"reason\": \"Sequence-to-sequence models are a specific type of encoder-decoder model. The encoder-decoder architecture is fundamental to sequence-to-sequence learning, providing the framework for mapping input sequences to output sequences.\", \"similarity\": 0.85, \"to\": \"Encoder-decoder model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Sequence-to-sequence model\", \"reason\": \"Recurrent Neural Networks (RNNs), especially LSTMs and GRUs, are commonly used as the building blocks within sequence-to-sequence models to handle sequential data. They are crucial for processing variable-length input and output sequences.\", \"similarity\": 0.8, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Sequence-to-sequence model\", \"reason\": \"Attention mechanisms are frequently integrated into sequence-to-sequence models to improve performance, particularly in handling long sequences. They allow the decoder to focus on relevant parts of the input sequence when generating the output.\", \"similarity\": 0.75, \"to\": \"Attention (machine learning)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Sequence-to-sequence model\", \"reason\": \"Text summarization, especially abstractive summarization, is another application of sequence-to-sequence models. The model learns to generate a concise summary of a longer text, similar to machine translation in its sequence transformation task.\", \"similarity\": 0.7, \"to\": \"Automatic summarization\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"BERT is based on the Transformer architecture. The Transformer provides the foundational building blocks for BERT\\u0027s self-attention mechanism and overall structure.\", \"similarity\": 0.95, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"BERT utilizes word embeddings to represent words as vectors, capturing semantic relationships. Word embeddings are a crucial component for BERT\\u0027s understanding of language.\", \"similarity\": 0.85, \"to\": \"Word embedding\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"BERT is a prominent model in the field of NLP, designed to solve various NLP tasks such as text classification, question answering, and sentiment analysis.\", \"similarity\": 0.8, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"BERT is pre-trained using a masked language modeling objective, where the model learns to predict masked words in a sentence. This is a core training technique for BERT.\", \"similarity\": 0.9, \"to\": \"Masked language model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"BERT (language model)\", \"reason\": \"GPT-3 is another large language model that, like BERT, leverages the Transformer architecture and is used for various NLP tasks. Both models represent significant advancements in language understanding and generation, although they differ in architecture and training objectives.\", \"similarity\": 0.75, \"to\": \"GPT-3\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3 is a prominent example of a large language model. It shares the core characteristics of being trained on massive datasets of text and code to generate human-quality text.\", \"similarity\": 0.95, \"to\": \"Large language model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3\\u0027s architecture is based on the Transformer model. The Transformer architecture is fundamental to its ability to process and generate text effectively.\", \"similarity\": 0.9, \"to\": \"Transformer (machine learning model)\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3 is a generative model, meaning it\\u0027s designed to generate new content (text, code, etc.) rather than simply classifying or predicting existing data. This generative capability is a defining feature.\", \"similarity\": 0.85, \"to\": \"Generative model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3 is a type of artificial neural network, specifically a deep neural network. It leverages the principles of neural networks to learn patterns and relationships in data.\", \"similarity\": 0.8, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"GPT-3\", \"reason\": \"GPT-3 is a significant advancement in the field of natural language processing (NLP). It demonstrates state-of-the-art performance in various NLP tasks, such as text generation, translation, and question answering.\", \"similarity\": 0.75, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"NLP is a closely related field that focuses on enabling computers to understand, interpret, and generate human language. Computational linguistics provides the theoretical foundations and computational tools for NLP.\", \"similarity\": 0.9, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Language technology is a broader term encompassing computational linguistics and NLP, along with other areas like speech recognition and machine translation. It represents the application of computational techniques to language-related problems.\", \"similarity\": 0.8, \"to\": \"Language technology\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Machine translation, the automatic translation of text from one language to another, is a major application area of computational linguistics. It relies heavily on computational linguistic techniques for parsing, semantic analysis, and language generation.\", \"similarity\": 0.7, \"to\": \"Machine translation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Corpus linguistics involves the study of language based on large collections of real-world text (corpora). Computational linguistics provides the tools and techniques for analyzing these corpora, extracting linguistic patterns, and building statistical language models.\", \"similarity\": 0.65, \"to\": \"Corpus linguistics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational linguistics\", \"reason\": \"Cognitive science is the interdisciplinary study of the mind and its processes. Computational linguistics contributes to cognitive science by providing computational models of language processing, which can help us understand how humans understand and produce language.\", \"similarity\": 0.6, \"to\": \"Cognitive science\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Machine learning is a subfield of artificial intelligence. AI is the broader concept of creating intelligent agents, while machine learning focuses on enabling systems to learn from data without explicit programming. Many AI systems leverage machine learning techniques.\", \"similarity\": 0.9, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Data mining and machine learning share the goal of extracting knowledge and patterns from data. Many data mining techniques are based on machine learning algorithms. However, data mining often involves a broader range of tasks, including data cleaning, transformation, and visualization, while machine learning focuses more specifically on model building and prediction.\", \"similarity\": 0.8, \"to\": \"Data mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Statistical learning is closely related to machine learning, with significant overlap in techniques and goals. Both fields focus on building models from data to make predictions or inferences. Statistical learning emphasizes the statistical foundations of these models, while machine learning often focuses more on algorithmic efficiency and performance.\", \"similarity\": 0.85, \"to\": \"Statistical learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Deep learning is a specific type of machine learning that uses artificial neural networks with multiple layers (deep neural networks) to analyze data. It has achieved significant success in areas like image recognition, natural language processing, and speech recognition. Deep learning is a subset of machine learning.\", \"similarity\": 0.75, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Machine learning\", \"reason\": \"Pattern recognition is the process of identifying regularities or patterns in data. Machine learning algorithms are frequently used for pattern recognition tasks, such as image classification, object detection, and speech recognition. Pattern recognition provides the problem domain for many machine learning applications.\", \"similarity\": 0.7, \"to\": \"Pattern recognition\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Machine learning is a core subfield of AI focused on enabling systems to learn from data without explicit programming. It\\u0027s a primary method for achieving AI.\", \"similarity\": 0.9, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Deep learning is a subfield of machine learning that uses artificial neural networks with multiple layers to analyze data and is a very popular and effective approach to AI.\", \"similarity\": 0.85, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Robotics often incorporates AI techniques to enable robots to perform complex tasks autonomously, such as navigation, object recognition, and manipulation. AI provides the \\u0027brains\\u0027 for many robots.\", \"similarity\": 0.75, \"to\": \"Robotics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"NLP is a branch of AI that deals with enabling computers to understand, interpret, and generate human language. It\\u0027s crucial for applications like chatbots, machine translation, and sentiment analysis.\", \"similarity\": 0.7, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Artificial intelligence\", \"reason\": \"Cognitive computing aims to simulate human thought processes in a computerized model. It overlaps significantly with AI, focusing on tasks that require human-like intelligence, such as problem-solving and decision-making.\", \"similarity\": 0.65, \"to\": \"Cognitive computing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"NLP is a core enabling technology for text mining, providing the tools to parse, understand, and extract meaning from text. Text mining often relies on NLP techniques for tasks like tokenization, part-of-speech tagging, and named entity recognition.\", \"similarity\": 0.9, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Both text mining and information retrieval deal with large amounts of text data. While information retrieval focuses on finding relevant documents, text mining goes a step further to discover patterns and knowledge within those documents. Text mining often uses information retrieval techniques to pre-select relevant documents.\", \"similarity\": 0.8, \"to\": \"Information retrieval\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Text mining is a subfield of data mining, specifically focused on extracting knowledge from textual data. The general principles and techniques of data mining, such as clustering, classification, and association rule mining, are applicable to text data.\", \"similarity\": 0.75, \"to\": \"Data mining\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Machine learning algorithms are widely used in text mining for tasks such as text classification, sentiment analysis, and topic modeling. Text mining often involves training machine learning models on text data to automate the process of knowledge discovery.\", \"similarity\": 0.7, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Text mining\", \"reason\": \"Computational linguistics provides the theoretical and computational foundations for understanding and processing human language. Text mining benefits from the linguistic insights and tools developed in computational linguistics, particularly for tasks like semantic analysis and discourse analysis.\", \"similarity\": 0.65, \"to\": \"Computational linguistics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Speech recognition\", \"reason\": \"Automatic speech recognition (ASR) is essentially synonymous with speech recognition, often used interchangeably. It refers to the process of a computer converting spoken words into text.\", \"similarity\": 0.95, \"to\": \"Automatic speech recognition\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Speech recognition\", \"reason\": \"Speech recognition is a crucial component of Natural Language Processing (NLP). NLP deals with enabling computers to understand and process human language, and speech recognition provides the initial step of converting spoken language into a machine-readable format for further analysis by NLP algorithms.\", \"similarity\": 0.85, \"to\": \"Natural language processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Speech recognition\", \"reason\": \"Speech recognition is a core technology enabling Voice User Interfaces (VUIs). VUIs allow users to interact with systems using voice commands, and speech recognition is the technology that interprets those commands.\", \"similarity\": 0.8, \"to\": \"Voice user interface\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Speech recognition\", \"reason\": \"While speech recognition focuses on *what* is being said, speaker recognition focuses on *who* is speaking. Both are related to analyzing audio of speech, but have different goals. Speaker recognition can be used in conjunction with speech recognition to improve accuracy or for authentication purposes.\", \"similarity\": 0.75, \"to\": \"Speaker recognition\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Speech recognition\", \"reason\": \"Acoustic phonetics is the study of the physical properties of speech sounds. Speech recognition systems rely heavily on the principles of acoustic phonetics to analyze and model the acoustic signals of speech and map them to phonemes and words.\", \"similarity\": 0.7, \"to\": \"Acoustic phonetics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Representation learning\", \"reason\": \"Representation learning is a subfield of machine learning focused on automatically discovering useful representations of data that make it easier to learn predictive models. It addresses the feature engineering bottleneck in traditional machine learning.\", \"similarity\": 0.85, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Representation learning\", \"reason\": \"Representation learning aims to automate the process of feature engineering, which is the manual creation of features from raw data. It seeks to learn these features directly from the data itself, reducing the need for human intervention.\", \"similarity\": 0.8, \"to\": \"Feature engineering\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Representation learning\", \"reason\": \"Deep learning is a specific type of representation learning that uses deep neural networks with multiple layers to learn hierarchical representations of data. Many state-of-the-art representation learning techniques are based on deep learning architectures.\", \"similarity\": 0.75, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Representation learning\", \"reason\": \"A significant portion of representation learning falls under unsupervised learning, where the goal is to learn useful representations from unlabeled data. Techniques like autoencoders and contrastive learning are used to discover underlying structure and patterns in the data without explicit supervision.\", \"similarity\": 0.7, \"to\": \"Unsupervised learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Representation learning\", \"reason\": \"Representation learning often involves reducing the dimensionality of data while preserving important information. Techniques like Principal Component Analysis (PCA) and autoencoders can be used to learn lower-dimensional representations that capture the essential features of the data.\", \"similarity\": 0.65, \"to\": \"Dimensionality reduction\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Backpropagation\", \"reason\": \"Backpropagation is a specific application of gradient descent used to train artificial neural networks. It calculates the gradient of the loss function with respect to the weights of the network, which is then used to update the weights in the direction that minimizes the loss.\", \"similarity\": 0.9, \"to\": \"Gradient descent\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Backpropagation\", \"reason\": \"Backpropagation is a special case of automatic differentiation (specifically, reverse accumulation) applied to neural networks. Automatic differentiation provides a general framework for computing derivatives of complex functions, and backpropagation leverages this to efficiently compute gradients in deep learning models.\", \"similarity\": 0.85, \"to\": \"Automatic differentiation\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Backpropagation\", \"reason\": \"Backpropagation relies heavily on the chain rule of calculus to compute the gradients through multiple layers of a neural network. The chain rule allows for the decomposition of the derivative of a composite function into the product of derivatives of its constituent functions.\", \"similarity\": 0.8, \"to\": \"Chain rule\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Backpropagation\", \"reason\": \"Backpropagation is the primary algorithm used to train most feedforward neural networks. It\\u0027s intrinsically linked to the architecture and learning process of these networks, enabling them to learn complex patterns from data.\", \"similarity\": 0.75, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Backpropagation\", \"reason\": \"Backpropagation aims to navigate the error surface (also known as the loss landscape) of a neural network to find the minimum, which corresponds to the optimal set of weights. Understanding the properties of the error surface is crucial for understanding the challenges and limitations of backpropagation.\", \"similarity\": 0.7, \"to\": \"Error surface\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Convolutional neural network\", \"reason\": \"Convolutional Neural Networks (CNNs) are a fundamental type of deep learning architecture. Deep learning encompasses a broader range of neural network architectures, but CNNs are a core component and often used as a building block in more complex deep learning models.\", \"similarity\": 0.85, \"to\": \"Deep learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Convolutional neural network\", \"reason\": \"CNNs are a specific type of artificial neural network. They share the same underlying principles of interconnected nodes (neurons) and learnable weights, but CNNs are specialized for processing grid-like data such as images or audio through the use of convolutional layers.\", \"similarity\": 0.8, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Convolutional neural network\", \"reason\": \"Image recognition is a primary application area for CNNs. CNNs have achieved state-of-the-art results in various image recognition tasks, including image classification, object detection, and image segmentation. The architecture of CNNs is specifically designed to extract relevant features from images.\", \"similarity\": 0.75, \"to\": \"Image recognition\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Convolutional neural network\", \"reason\": \"CNNs are a crucial tool in the field of computer vision. They enable computers to \\u0027see\\u0027 and interpret images and videos. Many computer vision tasks, such as object detection, image segmentation, and image classification, heavily rely on CNNs.\", \"similarity\": 0.7, \"to\": \"Computer vision\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Convolutional neural network\", \"reason\": \"While structurally different, both CNNs and RNNs are important types of neural networks used in deep learning. They both learn complex patterns from data, but CNNs are better suited for spatial data (like images) and RNNs are better suited for sequential data (like text or time series). They represent different approaches to feature extraction and pattern recognition within the broader field of neural networks.\", \"similarity\": 0.6, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Discriminative model\", \"reason\": \"Generative and discriminative models are two main approaches to statistical classification. They both aim to classify data, but differ in their approach. Generative models learn the joint probability distribution, while discriminative models learn the conditional probability distribution directly. They are often contrasted with each other.\", \"similarity\": 0.9, \"to\": \"Generative model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Discriminative model\", \"reason\": \"Discriminative models are primarily used for classification tasks. They directly learn the decision boundary between classes, making them a core component of classification algorithms.\", \"similarity\": 0.8, \"to\": \"Statistical classification\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Discriminative model\", \"reason\": \"Logistic regression is a classic example of a discriminative model. It directly models the probability of a class given the input features, without modeling the underlying data distribution.\", \"similarity\": 0.75, \"to\": \"Logistic regression\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Discriminative model\", \"reason\": \"SVMs are another prominent example of discriminative models. They focus on finding the optimal hyperplane that separates different classes in the feature space, directly learning the decision boundary.\", \"similarity\": 0.75, \"to\": \"Support vector machine\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Discriminative model\", \"reason\": \"CRFs are discriminative models used for structured prediction, such as sequence labeling. They model the conditional probability of a label sequence given an input sequence, making them a discriminative approach for tasks where dependencies between labels are important.\", \"similarity\": 0.7, \"to\": \"Conditional random field\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Variational autoencoder\", \"reason\": \"Variational autoencoders are a type of autoencoder. They share the same fundamental architecture (encoder and decoder) and goal (learning a compressed representation), but VAEs add a probabilistic element to the encoding.\", \"similarity\": 0.9, \"to\": \"Autoencoder\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Variational autoencoder\", \"reason\": \"VAEs are generative models, meaning they can be used to generate new data samples similar to the training data. This is a key characteristic and application of VAEs.\", \"similarity\": 0.8, \"to\": \"Generative model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Variational autoencoder\", \"reason\": \"VAEs are latent variable models because they learn a lower-dimensional latent space representation of the data. The encoder maps the input to a distribution in this latent space.\", \"similarity\": 0.75, \"to\": \"Latent variable model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Variational autoencoder\", \"reason\": \"VAEs can be viewed as a type of Bayesian network with a specific structure. They use probabilistic inference to learn the parameters of the model.\", \"similarity\": 0.65, \"to\": \"Bayesian network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Variational autoencoder\", \"reason\": \"Both VAEs and PCA are dimensionality reduction techniques. While PCA is linear, VAEs can learn non-linear representations, but the underlying goal of finding a lower-dimensional representation is shared.\", \"similarity\": 0.55, \"to\": \"Principal component analysis\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative adversarial network\", \"reason\": \"Both are generative models used for unsupervised learning, employing neural networks to learn latent representations and generate new data instances. Both also rely on training a model to reconstruct input data, though VAEs use a probabilistic encoder.\", \"similarity\": 0.85, \"to\": \"Variational autoencoder\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative adversarial network\", \"reason\": \"Autoencoders share the core concept of learning a compressed, latent representation of data. While not inherently generative like GANs, they form a building block for more complex generative models and can be adapted for generative purposes. GANs and Autoencoders both use neural networks to learn representations of data.\", \"similarity\": 0.75, \"to\": \"Autoencoder\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative adversarial network\", \"reason\": \"Generative adversarial networks are a type of generative model. The concept of a generative model is a superset of GANs. Both aim to learn the underlying probability distribution of a dataset to generate new, similar samples.\", \"similarity\": 0.9, \"to\": \"Generative model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative adversarial network\", \"reason\": \"GANs are implemented using neural networks. The generator and discriminator in a GAN are typically neural networks. The underlying mathematical and computational framework is shared.\", \"similarity\": 0.7, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Generative adversarial network\", \"reason\": \"GANs are a prime example of adversarial machine learning, where two models (generator and discriminator) compete against each other. The adversarial training process is a key characteristic of both concepts.\", \"similarity\": 0.8, \"to\": \"Adversarial machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Autoregressive model\", \"reason\": \"Both are stochastic models where the future state depends only on the present state (Markov property). Autoregressive models are a specific type of Markov model applied to time series data.\", \"similarity\": 0.85, \"to\": \"Markov model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Autoregressive model\", \"reason\": \"RNNs, particularly LSTMs and GRUs, are often used to implement autoregressive models. They process sequential data by maintaining a hidden state that represents past information, similar to how an autoregressive model uses past values to predict future values.\", \"similarity\": 0.8, \"to\": \"Recurrent neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Autoregressive model\", \"reason\": \"Autoregressive models are a fundamental tool in time series analysis for forecasting and understanding temporal dependencies in data. They are used to model the relationship between a variable and its past values.\", \"similarity\": 0.75, \"to\": \"Time series analysis\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Autoregressive model\", \"reason\": \"Moving-average models, along with autoregressive models, form the basis of ARMA (Autoregressive Moving Average) models. Both are linear time series models used for forecasting, but moving-average models use past error terms instead of past values of the time series itself.\", \"similarity\": 0.7, \"to\": \"Moving-average model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Autoregressive model\", \"reason\": \"Kalman filters can be used to estimate the parameters of autoregressive models and to predict future values of a time series. They provide a recursive algorithm for estimating the state of a dynamic system from a series of noisy measurements, which is relevant in the context of autoregressive modeling.\", \"similarity\": 0.65, \"to\": \"Kalman filter\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Markov chain\", \"reason\": \"HMMs are a generalization of Markov chains where the state is not directly observable, but rather inferred through a probability distribution of observations. They share the Markov property and are used for modeling sequential data.\", \"similarity\": 0.9, \"to\": \"Hidden Markov model\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Markov chain\", \"reason\": \"Markov process is a general term for a stochastic process that satisfies the Markov property (memorylessness). A Markov chain is a specific type of Markov process with a discrete state space.\", \"similarity\": 0.85, \"to\": \"Markov process\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Markov chain\", \"reason\": \"Markov chains are a specific type of stochastic process. Stochastic processes are mathematical models used to represent the evolution of random variables over time, and Markov chains are a subset where the future state depends only on the present state.\", \"similarity\": 0.75, \"to\": \"Stochastic process\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Markov chain\", \"reason\": \"While not directly a Markov chain, Bayesian networks can represent dependencies between variables, and in some cases, a Bayesian network can be structured to represent a Markov chain or a Hidden Markov Model. Both are probabilistic graphical models.\", \"similarity\": 0.65, \"to\": \"Bayesian network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Markov chain\", \"reason\": \"Queueing theory often uses Markov chains to model the state of a queue (e.g., the number of customers in the system). The arrival and service processes can be modeled as Markovian processes, allowing for analysis of queue performance.\", \"similarity\": 0.6, \"to\": \"Queueing theory\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Connectionism\", \"reason\": \"Artificial neural networks are the primary computational models used in connectionism. They share the core principles of distributed representation, parallel processing, and learning through adjusting connection weights.\", \"similarity\": 0.95, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Connectionism\", \"reason\": \"Connectionism is a significant approach within cognitive science, offering a computational framework for understanding mental processes. It contrasts with symbolic AI but aims to explain cognition.\", \"similarity\": 0.85, \"to\": \"Cognitive science\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Connectionism\", \"reason\": \"Parallel Distributed Processing (PDP) is often used synonymously with connectionism. It emphasizes the parallel nature of computation and the distributed representation of information across interconnected units.\", \"similarity\": 0.9, \"to\": \"Parallel distributed processing\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Connectionism\", \"reason\": \"Connectionist models, particularly neural networks, are a major part of machine learning. They learn from data by adjusting connection weights, enabling them to perform tasks without explicit programming.\", \"similarity\": 0.8, \"to\": \"Machine learning\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Connectionism\", \"reason\": \"Connectionism provides a computational framework that can be used to model and understand cognitive processes studied in cognitive psychology, such as memory, perception, and language processing. It offers an alternative to traditional information processing models.\", \"similarity\": 0.75, \"to\": \"Cognitive psychology\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational neuroscience\", \"reason\": \"Theoretical neuroscience is a closely related field that uses mathematical and computational tools to develop theories and models of the nervous system. It often overlaps with computational neuroscience, with the distinction being a greater emphasis on abstract models and theoretical frameworks rather than direct simulation of biological details.\", \"similarity\": 0.9, \"to\": \"Theoretical neuroscience\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational neuroscience\", \"reason\": \"Neuroinformatics focuses on the organization, storage, analysis, and sharing of neuroscience data. It provides the infrastructure and tools necessary for computational neuroscience research, including databases, software, and standards for data exchange and analysis. Many computational neuroscientists rely on neuroinformatics resources.\", \"similarity\": 0.8, \"to\": \"Neuroinformatics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational neuroscience\", \"reason\": \"Cognitive neuroscience investigates the neural basis of cognitive functions. Computational models are increasingly used in cognitive neuroscience to understand how the brain implements cognitive processes such as perception, attention, memory, and decision-making. Computational neuroscience provides tools and techniques for modeling these processes at different levels of abstraction.\", \"similarity\": 0.75, \"to\": \"Cognitive neuroscience\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational neuroscience\", \"reason\": \"Artificial neural networks are computational models inspired by the structure and function of biological neural networks. They are a key tool in computational neuroscience for simulating and understanding neural computation. While ANNs are often simplified compared to biological networks, they provide valuable insights into how neural networks can perform complex tasks.\", \"similarity\": 0.7, \"to\": \"Artificial neural network\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Computational neuroscience\", \"reason\": \"Mathematical neuroscience uses mathematical techniques to model and analyze neural systems. It is highly related to computational neuroscience, and the terms are sometimes used interchangeably. However, mathematical neuroscience may place a greater emphasis on analytical solutions and mathematical rigor, while computational neuroscience focuses more on simulations and numerical methods.\", \"similarity\": 0.65, \"to\": \"Mathematical neuroscience\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Cognitive science\", \"reason\": \"Psychology is a broad field encompassing the study of the mind and behavior, sharing significant overlap with cognitive science in areas like perception, memory, learning, and decision-making. Cognitive psychology is a major subfield of psychology that directly informs cognitive science.\", \"similarity\": 0.85, \"to\": \"Psychology\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Cognitive science\", \"reason\": \"Neuroscience provides the biological basis for cognitive processes. Cognitive neuroscience is a subfield that explicitly links brain activity to cognitive functions, making it highly relevant to understanding the mechanisms underlying cognition.\", \"similarity\": 0.8, \"to\": \"Neuroscience\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Cognitive science\", \"reason\": \"AI aims to create intelligent systems, often drawing inspiration from cognitive processes. Cognitive science provides theoretical frameworks and insights into human intelligence that can be applied to AI development. Conversely, AI models can serve as computational models for cognitive theories.\", \"similarity\": 0.75, \"to\": \"Artificial intelligence\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Cognitive science\", \"reason\": \"Linguistics studies language, a core cognitive function. Cognitive linguistics explores the relationship between language, mind, and experience. Understanding language processing is crucial for understanding human cognition.\", \"similarity\": 0.7, \"to\": \"Linguistics\", \"width\": 1}, {\"arrows\": \"to\", \"from\": \"Cognitive science\", \"reason\": \"Philosophy of mind explores fundamental questions about the nature of consciousness, mental states, and the mind-body problem. These philosophical inquiries provide a conceptual foundation for cognitive science and help to frame research questions.\", \"similarity\": 0.65, \"to\": \"Philosophy of mind\", \"width\": 1}]);\n",
       "\n",
       "                  nodeColors = {};\n",
       "                  allNodes = nodes.get({ returnType: \"Object\" });\n",
       "                  for (nodeId in allNodes) {\n",
       "                    nodeColors[nodeId] = allNodes[nodeId].color;\n",
       "                  }\n",
       "                  allEdges = edges.get({ returnType: \"Object\" });\n",
       "                  // adding nodes and edges to the graph\n",
       "                  data = {nodes: nodes, edges: edges};\n",
       "\n",
       "                  var options = {\n",
       "    \"configure\": {\n",
       "        \"enabled\": false\n",
       "    },\n",
       "    \"edges\": {\n",
       "        \"color\": {\n",
       "            \"inherit\": true\n",
       "        },\n",
       "        \"smooth\": {\n",
       "            \"enabled\": true,\n",
       "            \"type\": \"dynamic\"\n",
       "        }\n",
       "    },\n",
       "    \"interaction\": {\n",
       "        \"dragNodes\": true,\n",
       "        \"hideEdgesOnDrag\": false,\n",
       "        \"hideNodesOnDrag\": false\n",
       "    },\n",
       "    \"physics\": {\n",
       "        \"enabled\": true,\n",
       "        \"forceAtlas2Based\": {\n",
       "            \"avoidOverlap\": 0,\n",
       "            \"centralGravity\": 0.01,\n",
       "            \"damping\": 0.4,\n",
       "            \"gravitationalConstant\": -50,\n",
       "            \"springConstant\": 0.03,\n",
       "            \"springLength\": 100\n",
       "        },\n",
       "        \"solver\": \"forceAtlas2Based\",\n",
       "        \"stabilization\": {\n",
       "            \"enabled\": true,\n",
       "            \"fit\": true,\n",
       "            \"iterations\": 1000,\n",
       "            \"onlyDynamicEdges\": false,\n",
       "            \"updateInterval\": 50\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  network = new vis.Network(container, data, options);\n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "\n",
       "                  \n",
       "                  // make a custom popup\n",
       "                      var popup = document.createElement(\"div\");\n",
       "                      popup.className = 'popup';\n",
       "                      popupTimeout = null;\n",
       "                      popup.addEventListener('mouseover', function () {\n",
       "                          console.log(popup)\n",
       "                          if (popupTimeout !== null) {\n",
       "                              clearTimeout(popupTimeout);\n",
       "                              popupTimeout = null;\n",
       "                          }\n",
       "                      });\n",
       "                      popup.addEventListener('mouseout', function () {\n",
       "                          if (popupTimeout === null) {\n",
       "                              hidePopup();\n",
       "                          }\n",
       "                      });\n",
       "                      container.appendChild(popup);\n",
       "\n",
       "\n",
       "                      // use the popup event to show\n",
       "                      network.on(\"showPopup\", function (params) {\n",
       "                          showPopup(params);\n",
       "                      });\n",
       "\n",
       "                      // use the hide event to hide it\n",
       "                      network.on(\"hidePopup\", function (params) {\n",
       "                          hidePopup();\n",
       "                      });\n",
       "\n",
       "                      // hiding the popup through css\n",
       "                      function hidePopup() {\n",
       "                          popupTimeout = setTimeout(function () { popup.style.display = 'none'; }, 500);\n",
       "                      }\n",
       "\n",
       "                      // showing the popup\n",
       "                      function showPopup(nodeId) {\n",
       "                          // get the data from the vis.DataSet\n",
       "                          var nodeData = nodes.get([nodeId]);\n",
       "                          popup.innerHTML = nodeData[0].title;\n",
       "\n",
       "                          // get the position of the node\n",
       "                          var posCanvas = network.getPositions([nodeId])[nodeId];\n",
       "\n",
       "                          // get the bounding box of the node\n",
       "                          var boundingBox = network.getBoundingBox(nodeId);\n",
       "\n",
       "                          //position tooltip:\n",
       "                          posCanvas.x = posCanvas.x + 0.5 * (boundingBox.right - boundingBox.left);\n",
       "\n",
       "                          // convert coordinates to the DOM space\n",
       "                          var posDOM = network.canvasToDOM(posCanvas);\n",
       "\n",
       "                          // Give it an offset\n",
       "                          posDOM.x += 10;\n",
       "                          posDOM.y -= 20;\n",
       "\n",
       "                          // show and place the tooltip.\n",
       "                          popup.style.display = 'block';\n",
       "                          popup.style.top = posDOM.y + 'px';\n",
       "                          popup.style.left = posDOM.x + 'px';\n",
       "                      }\n",
       "                  \n",
       "\n",
       "\n",
       "                  \n",
       "\n",
       "                  return network;\n",
       "\n",
       "              }\n",
       "              drawGraph();\n",
       "        </script>\n",
       "    </body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Display pyviz network\n",
    "nt.save_graph(\"llmgraph.html\")\n",
    "IPython.display.HTML(filename=\"llmgraph.html\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a152fea-a796-4e6f-9d39-2b101d27d981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
